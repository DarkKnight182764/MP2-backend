{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# check version of keras_vggface\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "import keras_vggface\n",
    "# print version\n",
    "print(keras_vggface.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "# create a vggface model\n",
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dir = join(os.getcwd(), \"grid\")\n",
    "speakers = listdir(grid_dir)\n",
    "vocab = list()\n",
    "aligns = dict()\n",
    "\n",
    "for speaker in speakers:\n",
    "    speaker_align_path = join(grid_dir, speaker, \"align\")\n",
    "    speaker_vid_names = listdir(speaker_align_path)\n",
    "    for speaker_vid_name in speaker_vid_names:\n",
    "        df = pd.read_csv(join(speaker_align_path, speaker_vid_name), \n",
    "                         delimiter = \" \", names=[\"start\", \"end\", \"utter\"])        \n",
    "        for row in np.array(df):            \n",
    "            if row[2] not in vocab:\n",
    "                vocab.append(row[2])        \n",
    "        aligns[speaker_vid_name.replace(\".align\", \"\")] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sil', 'bin', 'blue', 'at', 'f', 'two', 'now', 'three', 'soon', 'four', 'please', 'five', 'again', 'l', 'six', 'seven', 'eight', 'nine', 's', 'one', 'zero', 'z', 'by', 'm', 'in', 'e', 'r', 'sp', 'with', 'g', 't', 'green', 'a', 'h', 'n', 'u', 'b', 'o', 'i', 'red', 'white', 'lay', 'd', 'k', 'q', 'x', 'y', 'j', 'place', 'c', 'p', 'v', 'set']\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(len(vocab))\n",
    "# print(aligns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open(\"train_dump_x_y\", \"rb\") as dill_file:\n",
    "    X, Y = dill.load(dill_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open(\"vocab_dump\", \"wb\") as f:\n",
    "    dill.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614400000, 31800000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.nbytes, Y.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h1 style=\"text-align:center;\">PREPROCESSING DONE</h1><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ends=aligns[\"bbaf2n\"][\"end\"]\n",
    "for i in ends:\n",
    "    print(i/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras_vggface.utils import preprocess_input\n",
    "from mtcnn import MTCNN\n",
    "import matplotlib.pyplot as  plt\n",
    "%matplotlib inline\n",
    "\n",
    "detector = MTCNN()\n",
    "grid_dir = os.path.join(os.getcwd(), \"grid\")\n",
    "speakers = os.listdir(grid_dir)\n",
    "X = np.zeros((len(speakers), 1000, 75, 2048), dtype=np.float32)\n",
    "Y = np.zeros((len(speakers), 1000, 75, len(vocab)))\n",
    "\n",
    "for num_speaker, speaker in enumerate(speakers):    \n",
    "    speaker_vids_path = os.path.join(grid_dir, speaker, speaker)\n",
    "    speaker_vids = os.listdir(speaker_vids_path)\n",
    "    \n",
    "    for i, speaker_vid in enumerate(speaker_vids):\n",
    "        speaker_vids[i] = speaker_vid.replace(\".mpg\", \"\")\n",
    "        \n",
    "    for num_vid, speaker_vid in enumerate(speaker_vids):\n",
    "        df = aligns[speaker_vid]\n",
    "        starts = df[\"start\"]\n",
    "        ends = df[\"end\"]\n",
    "        utters = df[\"utter\"]\n",
    "        if num_vid%20 == 0:\n",
    "            print(num_vid)\n",
    "        cam = cv2.VideoCapture(os.path.join(speaker_vids_path, speaker_vid+\".mpg\")) \n",
    "        frame_buffer = np.zeros((75, 224, 224, 3), dtype=np.float32)\n",
    "        for num_frame in range(75):                       \n",
    "            ret, frame = cam.read()              \n",
    "            if frame is not None:\n",
    "                frame=frame.astype(np.float32)  \n",
    "#                 frame/=255\n",
    "#                 plt.imshow(frame)\n",
    "#                 plt.show()\n",
    "#                 frame*=255\n",
    "                #frame=frame/255    \n",
    "                results = detector.detect_faces(frame)\n",
    "                if len(results) >= 1:\n",
    "                    x1, y1, width, height = results[0]['box']\n",
    "                    x2, y2 = x1 + width, y1 + height\n",
    "                    frame = frame[y1:y2, x1:x2]\n",
    "#                 frame/=255\n",
    "#                 plt.imshow(frame)\n",
    "#                 plt.show()\n",
    "#                 frame*=255\n",
    "#                 input()\n",
    "                frame = cv2.resize(frame, (224, 224)).astype(np.float32)                             \n",
    "                \n",
    "                #frame = np.reshape(frame, (1, 224, 224, 3))\n",
    "                #frame = np.expand_dims(frame, axis = 0)\n",
    "                frame_buffer[num_frame] = frame\n",
    "\n",
    "            frame_utter = None\n",
    "            for i_end, end in enumerate(ends):                \n",
    "                if num_frame < end/1000:\n",
    "                    frame_utter = utters[i_end]\n",
    "                    break            \n",
    "            assert frame_utter\n",
    "            Y[num_speaker, num_vid, num_frame, vocab.index(frame_utter)] = 1\n",
    "            \n",
    "        frame_buffer = preprocess_input(frame_buffer)\n",
    "        X[num_speaker, num_vid] = model.predict(frame_buffer)        \n",
    "                                                         \n",
    "# print(speaker_vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(\"train_dump_x_y\", \"wb\") as dill_file:\n",
    "    dill.dump((X,Y), dill_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.utils import preprocess_input\n",
    "x = preprocess_input(np.expand_dims(frame_buffer[19]*255, axis=0), version=2)\n",
    "p = model.predict(x)\n",
    "num=0\n",
    "for ip in p[0]:\n",
    "    if not ip == 0:\n",
    "        num+=1\n",
    "        print(ip)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=X[0,6,0]\n",
    "num=0\n",
    "for ip in p:\n",
    "    if not ip == 0:\n",
    "        num+=1\n",
    "        print(ip)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[0,969,29]\n",
    "# vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 75, 2048),\n",
       " (700, 75, 53),\n",
       " (201, 75, 2048),\n",
       " (201, 75, 53),\n",
       " (99, 75, 2048),\n",
       " (99, 75, 53))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X[0], Y[0],\n",
    "                                                    test_size=0.3, random_state=69)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test,\n",
    "                                                    test_size=0.33, random_state=69)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm_5/Identity:0\", shape=(None, 75, 1024), dtype=float32)\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 75, 2048)]        0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 75, 1024)          12587008  \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 75, 512)           524800    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 75, 128)           65664     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 75, 53)            6837      \n",
      "=================================================================\n",
      "Total params: 13,184,309\n",
      "Trainable params: 13,184,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 700 samples\n",
      "Epoch 1/200\n",
      "700/700 [==============================] - 3s 5ms/sample - loss: 2.7373 - accuracy: 0.4623\n",
      "Epoch 2/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 2.4048 - accuracy: 0.5077\n",
      "Epoch 3/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 2.2949 - accuracy: 0.5077\n",
      "Epoch 4/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 2.1005 - accuracy: 0.5087\n",
      "Epoch 5/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.9911 - accuracy: 0.5151\n",
      "Epoch 6/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.9243 - accuracy: 0.5257\n",
      "Epoch 7/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.8619 - accuracy: 0.5330\n",
      "Epoch 8/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.8095 - accuracy: 0.5395\n",
      "Epoch 9/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.7574 - accuracy: 0.5495\n",
      "Epoch 10/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.7041 - accuracy: 0.5594\n",
      "Epoch 11/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.7082 - accuracy: 0.5538\n",
      "Epoch 12/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.6228 - accuracy: 0.5673\n",
      "Epoch 13/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.5661 - accuracy: 0.5741\n",
      "Epoch 14/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.5097 - accuracy: 0.5822\n",
      "Epoch 15/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.4676 - accuracy: 0.5894\n",
      "Epoch 16/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.4324 - accuracy: 0.5953\n",
      "Epoch 17/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.4520 - accuracy: 0.5929\n",
      "Epoch 18/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.4052 - accuracy: 0.6046\n",
      "Epoch 19/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.3412 - accuracy: 0.6191\n",
      "Epoch 20/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.2977 - accuracy: 0.6317\n",
      "Epoch 21/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.2571 - accuracy: 0.6430\n",
      "Epoch 22/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.2386 - accuracy: 0.6483\n",
      "Epoch 23/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.2060 - accuracy: 0.6568\n",
      "Epoch 24/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.1613 - accuracy: 0.6702\n",
      "Epoch 25/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.1372 - accuracy: 0.6774\n",
      "Epoch 26/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.1252 - accuracy: 0.6812\n",
      "Epoch 27/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.1046 - accuracy: 0.6841\n",
      "Epoch 28/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.0786 - accuracy: 0.6886\n",
      "Epoch 29/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.0741 - accuracy: 0.6898\n",
      "Epoch 30/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.0669 - accuracy: 0.6933\n",
      "Epoch 31/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.0753 - accuracy: 0.6955\n",
      "Epoch 32/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 1.0282 - accuracy: 0.7050\n",
      "Epoch 33/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.9877 - accuracy: 0.7161\n",
      "Epoch 34/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.9935 - accuracy: 0.7137\n",
      "Epoch 35/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.9813 - accuracy: 0.7151\n",
      "Epoch 36/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.9396 - accuracy: 0.7286\n",
      "Epoch 37/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.9193 - accuracy: 0.7333\n",
      "Epoch 38/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.9306 - accuracy: 0.7313\n",
      "Epoch 39/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.9389 - accuracy: 0.7247\n",
      "Epoch 40/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.9216 - accuracy: 0.7300\n",
      "Epoch 41/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.9003 - accuracy: 0.7392\n",
      "Epoch 42/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8847 - accuracy: 0.7405\n",
      "Epoch 43/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8668 - accuracy: 0.7440\n",
      "Epoch 44/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8508 - accuracy: 0.7501\n",
      "Epoch 45/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8495 - accuracy: 0.7499\n",
      "Epoch 46/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8623 - accuracy: 0.7473\n",
      "Epoch 47/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8866 - accuracy: 0.7396\n",
      "Epoch 48/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8328 - accuracy: 0.7540\n",
      "Epoch 49/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8223 - accuracy: 0.7584\n",
      "Epoch 50/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8001 - accuracy: 0.7641\n",
      "Epoch 51/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8158 - accuracy: 0.7586\n",
      "Epoch 52/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8077 - accuracy: 0.7577\n",
      "Epoch 53/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8574 - accuracy: 0.7456\n",
      "Epoch 54/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.8392 - accuracy: 0.7519\n",
      "Epoch 55/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7722 - accuracy: 0.7702\n",
      "Epoch 56/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7731 - accuracy: 0.7691\n",
      "Epoch 57/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7748 - accuracy: 0.7675\n",
      "Epoch 58/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7606 - accuracy: 0.7738\n",
      "Epoch 59/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7362 - accuracy: 0.7814\n",
      "Epoch 60/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7484 - accuracy: 0.7742\n",
      "Epoch 61/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7641 - accuracy: 0.7709\n",
      "Epoch 62/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7930 - accuracy: 0.7641\n",
      "Epoch 63/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7457 - accuracy: 0.7751\n",
      "Epoch 64/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7103 - accuracy: 0.7880\n",
      "Epoch 65/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7265 - accuracy: 0.7836\n",
      "Epoch 66/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7134 - accuracy: 0.7866\n",
      "Epoch 67/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7675 - accuracy: 0.7709\n",
      "Epoch 68/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7283 - accuracy: 0.7806\n",
      "Epoch 69/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6970 - accuracy: 0.7913\n",
      "Epoch 70/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6873 - accuracy: 0.7947\n",
      "Epoch 71/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7046 - accuracy: 0.7890\n",
      "Epoch 72/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.7119 - accuracy: 0.7864\n",
      "Epoch 73/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6996 - accuracy: 0.7886\n",
      "Epoch 74/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6915 - accuracy: 0.7918\n",
      "Epoch 75/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6966 - accuracy: 0.7891\n",
      "Epoch 76/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6975 - accuracy: 0.7905\n",
      "Epoch 77/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6767 - accuracy: 0.7962\n",
      "Epoch 78/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6905 - accuracy: 0.7924\n",
      "Epoch 79/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6728 - accuracy: 0.7968\n",
      "Epoch 80/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6510 - accuracy: 0.8043\n",
      "Epoch 81/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6545 - accuracy: 0.8028\n",
      "Epoch 82/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6264 - accuracy: 0.8124\n",
      "Epoch 83/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6391 - accuracy: 0.8083\n",
      "Epoch 84/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6374 - accuracy: 0.8079\n",
      "Epoch 85/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6444 - accuracy: 0.8036\n",
      "Epoch 86/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6485 - accuracy: 0.8030\n",
      "Epoch 87/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6394 - accuracy: 0.8068\n",
      "Epoch 88/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6075 - accuracy: 0.8171\n",
      "Epoch 89/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6088 - accuracy: 0.8170\n",
      "Epoch 90/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6314 - accuracy: 0.8102\n",
      "Epoch 91/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6135 - accuracy: 0.8146\n",
      "Epoch 92/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6130 - accuracy: 0.8129\n",
      "Epoch 93/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5887 - accuracy: 0.8219\n",
      "Epoch 94/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5922 - accuracy: 0.8195\n",
      "Epoch 95/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5865 - accuracy: 0.8217\n",
      "Epoch 96/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5853 - accuracy: 0.8216\n",
      "Epoch 97/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5876 - accuracy: 0.8233\n",
      "Epoch 98/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.6007 - accuracy: 0.8184\n",
      "Epoch 99/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5952 - accuracy: 0.8190\n",
      "Epoch 100/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5951 - accuracy: 0.8195\n",
      "Epoch 101/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5810 - accuracy: 0.8237\n",
      "Epoch 102/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5747 - accuracy: 0.8241\n",
      "Epoch 103/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5606 - accuracy: 0.8308\n",
      "Epoch 104/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5579 - accuracy: 0.8308\n",
      "Epoch 105/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5834 - accuracy: 0.8242\n",
      "Epoch 106/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5674 - accuracy: 0.8274\n",
      "Epoch 107/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5731 - accuracy: 0.8255\n",
      "Epoch 108/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5622 - accuracy: 0.8296\n",
      "Epoch 109/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5577 - accuracy: 0.8302\n",
      "Epoch 110/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5431 - accuracy: 0.8338\n",
      "Epoch 111/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5488 - accuracy: 0.8323\n",
      "Epoch 112/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5406 - accuracy: 0.8358\n",
      "Epoch 113/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5619 - accuracy: 0.8300\n",
      "Epoch 114/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5460 - accuracy: 0.8319\n",
      "Epoch 115/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5519 - accuracy: 0.8301\n",
      "Epoch 116/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5346 - accuracy: 0.8368\n",
      "Epoch 117/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5412 - accuracy: 0.8342\n",
      "Epoch 118/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5763 - accuracy: 0.8224\n",
      "Epoch 119/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5597 - accuracy: 0.8292\n",
      "Epoch 120/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5445 - accuracy: 0.8330\n",
      "Epoch 121/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5331 - accuracy: 0.8363\n",
      "Epoch 122/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5259 - accuracy: 0.8389\n",
      "Epoch 123/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5670 - accuracy: 0.8281\n",
      "Epoch 124/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5305 - accuracy: 0.8363\n",
      "Epoch 125/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5109 - accuracy: 0.8434\n",
      "Epoch 126/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5178 - accuracy: 0.8411\n",
      "Epoch 127/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5074 - accuracy: 0.8449\n",
      "Epoch 128/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5200 - accuracy: 0.8404\n",
      "Epoch 129/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5103 - accuracy: 0.8418\n",
      "Epoch 130/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5108 - accuracy: 0.8443\n",
      "Epoch 131/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4946 - accuracy: 0.8486\n",
      "Epoch 132/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4847 - accuracy: 0.8513\n",
      "Epoch 133/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4973 - accuracy: 0.8479\n",
      "Epoch 134/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4968 - accuracy: 0.8475\n",
      "Epoch 135/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4949 - accuracy: 0.8476\n",
      "Epoch 136/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4832 - accuracy: 0.8524\n",
      "Epoch 137/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5136 - accuracy: 0.8422\n",
      "Epoch 138/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5752 - accuracy: 0.8281\n",
      "Epoch 139/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5349 - accuracy: 0.8372\n",
      "Epoch 140/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4924 - accuracy: 0.8482\n",
      "Epoch 141/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4748 - accuracy: 0.8533\n",
      "Epoch 142/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4697 - accuracy: 0.8560\n",
      "Epoch 143/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4625 - accuracy: 0.8578\n",
      "Epoch 144/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4594 - accuracy: 0.8591\n",
      "Epoch 145/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4691 - accuracy: 0.8563\n",
      "Epoch 146/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4869 - accuracy: 0.8501\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4593 - accuracy: 0.8582\n",
      "Epoch 148/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4949 - accuracy: 0.8483\n",
      "Epoch 149/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4679 - accuracy: 0.8552\n",
      "Epoch 150/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5022 - accuracy: 0.8435\n",
      "Epoch 151/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4816 - accuracy: 0.8492\n",
      "Epoch 152/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4742 - accuracy: 0.8522\n",
      "Epoch 153/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4552 - accuracy: 0.8593\n",
      "Epoch 154/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4667 - accuracy: 0.8546\n",
      "Epoch 155/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4929 - accuracy: 0.8460\n",
      "Epoch 156/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4723 - accuracy: 0.8539\n",
      "Epoch 157/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4603 - accuracy: 0.8579\n",
      "Epoch 158/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4842 - accuracy: 0.8500\n",
      "Epoch 159/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4768 - accuracy: 0.8520\n",
      "Epoch 160/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4448 - accuracy: 0.8624\n",
      "Epoch 161/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4779 - accuracy: 0.8502\n",
      "Epoch 162/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4648 - accuracy: 0.8554\n",
      "Epoch 163/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4620 - accuracy: 0.8552\n",
      "Epoch 164/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4629 - accuracy: 0.8549\n",
      "Epoch 165/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4548 - accuracy: 0.8584\n",
      "Epoch 166/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4503 - accuracy: 0.8596\n",
      "Epoch 167/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4688 - accuracy: 0.8535\n",
      "Epoch 168/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5068 - accuracy: 0.8439\n",
      "Epoch 169/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.5129 - accuracy: 0.8388\n",
      "Epoch 170/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4502 - accuracy: 0.8594\n",
      "Epoch 171/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4392 - accuracy: 0.8635\n",
      "Epoch 172/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4315 - accuracy: 0.8668\n",
      "Epoch 173/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4201 - accuracy: 0.8709\n",
      "Epoch 174/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4240 - accuracy: 0.8699\n",
      "Epoch 175/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4196 - accuracy: 0.8703\n",
      "Epoch 176/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4381 - accuracy: 0.8652\n",
      "Epoch 177/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4629 - accuracy: 0.8548\n",
      "Epoch 178/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4364 - accuracy: 0.8631\n",
      "Epoch 179/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4165 - accuracy: 0.8715\n",
      "Epoch 180/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4246 - accuracy: 0.8672\n",
      "Epoch 181/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4368 - accuracy: 0.8634\n",
      "Epoch 182/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4356 - accuracy: 0.8636\n",
      "Epoch 183/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4324 - accuracy: 0.8622\n",
      "Epoch 184/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4273 - accuracy: 0.8669\n",
      "Epoch 185/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4170 - accuracy: 0.8711\n",
      "Epoch 186/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4141 - accuracy: 0.8711\n",
      "Epoch 187/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4185 - accuracy: 0.8685\n",
      "Epoch 188/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4004 - accuracy: 0.8749\n",
      "Epoch 189/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.3855 - accuracy: 0.8806\n",
      "Epoch 190/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.3902 - accuracy: 0.8786\n",
      "Epoch 191/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.3982 - accuracy: 0.8754\n",
      "Epoch 192/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4047 - accuracy: 0.8731\n",
      "Epoch 193/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.3996 - accuracy: 0.8758\n",
      "Epoch 194/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.3798 - accuracy: 0.8826\n",
      "Epoch 195/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4120 - accuracy: 0.8708\n",
      "Epoch 196/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4156 - accuracy: 0.8705\n",
      "Epoch 197/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4441 - accuracy: 0.8616\n",
      "Epoch 198/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4199 - accuracy: 0.8688\n",
      "Epoch 199/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.4050 - accuracy: 0.8716\n",
      "Epoch 200/200\n",
      "700/700 [==============================] - 1s 2ms/sample - loss: 0.3868 - accuracy: 0.8798\n",
      "Tensor(\"dense_31/Identity:0\", shape=(None, 75, 53), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "ip = keras.layers.Input(shape = (75, 2048), batch_size = None)\n",
    "a1 = keras.layers.LSTM(1024, return_sequences=True)(ip)\n",
    "print(a1)\n",
    "a2 = keras.layers.Dense(512, activation=\"relu\")(a1)\n",
    "a3 = keras.layers.Dense(128, activation=\"tanh\")(a2)\n",
    "# op = keras.layers.Dense(len(vocab), activcation=\"softmax\")(a3)\n",
    "op = keras.layers.Dense(53, activation=\"softmax\")(a3)\n",
    "\n",
    "model1=keras.models.Model(inputs=ip, outputs=op)\n",
    "model1.summary()\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model1.fit(x_train,y_train, batch_size=64, epochs=200)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 75, 2048)]        0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 75, 2048)          0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 75, 512)           1049088   \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 75, 512)           0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 75, 256)           131328    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 75, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 75, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 75, 256)           0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 75, 256)           65792     \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 75, 256)           0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 75, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 75, 53)            6837      \n",
      "=================================================================\n",
      "Total params: 1,680,181\n",
      "Trainable params: 1,680,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 700 samples\n",
      "Epoch 1/150\n",
      "700/700 [==============================] - 4s 6ms/sample - loss: 2.9475 - accuracy: 0.4458\n",
      "Epoch 2/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 2.5177 - accuracy: 0.5077\n",
      "Epoch 3/150\n",
      "700/700 [==============================] - 0s 599us/sample - loss: 2.4677 - accuracy: 0.5077\n",
      "Epoch 4/150\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 2.4318 - accuracy: 0.5077\n",
      "Epoch 5/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 2.3083 - accuracy: 0.5077\n",
      "Epoch 6/150\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 2.0766 - accuracy: 0.5077\n",
      "Epoch 7/150\n",
      "700/700 [==============================] - 0s 609us/sample - loss: 1.9724 - accuracy: 0.5094\n",
      "Epoch 8/150\n",
      "700/700 [==============================] - 0s 630us/sample - loss: 1.8897 - accuracy: 0.5194\n",
      "Epoch 9/150\n",
      "700/700 [==============================] - 0s 605us/sample - loss: 1.8026 - accuracy: 0.5339\n",
      "Epoch 10/150\n",
      "700/700 [==============================] - 0s 618us/sample - loss: 1.6662 - accuracy: 0.5453\n",
      "Epoch 11/150\n",
      "700/700 [==============================] - 0s 619us/sample - loss: 1.5347 - accuracy: 0.5588\n",
      "Epoch 12/150\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 1.4853 - accuracy: 0.5613\n",
      "Epoch 13/150\n",
      "700/700 [==============================] - 0s 640us/sample - loss: 1.4558 - accuracy: 0.5650\n",
      "Epoch 14/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 1.3552 - accuracy: 0.5843\n",
      "Epoch 15/150\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 1.2822 - accuracy: 0.5990\n",
      "Epoch 16/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 1.2125 - accuracy: 0.6208\n",
      "Epoch 17/150\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 1.1390 - accuracy: 0.6484\n",
      "Epoch 18/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 1.0706 - accuracy: 0.6705\n",
      "Epoch 19/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.9735 - accuracy: 0.7017\n",
      "Epoch 20/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.9124 - accuracy: 0.7177\n",
      "Epoch 21/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.8680 - accuracy: 0.7334\n",
      "Epoch 22/150\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.7826 - accuracy: 0.7590\n",
      "Epoch 23/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.7213 - accuracy: 0.7749\n",
      "Epoch 24/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.6911 - accuracy: 0.7840\n",
      "Epoch 25/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.6641 - accuracy: 0.7927\n",
      "Epoch 26/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.6371 - accuracy: 0.8019\n",
      "Epoch 27/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.6188 - accuracy: 0.8067\n",
      "Epoch 28/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.5833 - accuracy: 0.8154\n",
      "Epoch 29/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.5700 - accuracy: 0.8221\n",
      "Epoch 30/150\n",
      "700/700 [==============================] - 0s 622us/sample - loss: 0.5445 - accuracy: 0.8290\n",
      "Epoch 31/150\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 0.5259 - accuracy: 0.8350\n",
      "Epoch 32/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.5188 - accuracy: 0.8372\n",
      "Epoch 33/150\n",
      "700/700 [==============================] - 0s 571us/sample - loss: 0.5059 - accuracy: 0.8402\n",
      "Epoch 34/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.4841 - accuracy: 0.8466\n",
      "Epoch 35/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.4793 - accuracy: 0.8465\n",
      "Epoch 36/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.4668 - accuracy: 0.8518\n",
      "Epoch 37/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.4419 - accuracy: 0.8576\n",
      "Epoch 38/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.4291 - accuracy: 0.8627\n",
      "Epoch 39/150\n",
      "700/700 [==============================] - 0s 599us/sample - loss: 0.4220 - accuracy: 0.8645\n",
      "Epoch 40/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.4099 - accuracy: 0.8676\n",
      "Epoch 41/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.3999 - accuracy: 0.8720\n",
      "Epoch 42/150\n",
      "700/700 [==============================] - 0s 571us/sample - loss: 0.3929 - accuracy: 0.8743\n",
      "Epoch 43/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.3873 - accuracy: 0.8759\n",
      "Epoch 44/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.3865 - accuracy: 0.8752\n",
      "Epoch 45/150\n",
      "700/700 [==============================] - 0s 601us/sample - loss: 0.3754 - accuracy: 0.8786\n",
      "Epoch 46/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.3669 - accuracy: 0.8806\n",
      "Epoch 47/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.3537 - accuracy: 0.8847\n",
      "Epoch 48/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.3461 - accuracy: 0.8872\n",
      "Epoch 49/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.3414 - accuracy: 0.8877\n",
      "Epoch 50/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.3365 - accuracy: 0.8895\n",
      "Epoch 51/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.3271 - accuracy: 0.8913\n",
      "Epoch 52/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.3282 - accuracy: 0.8931\n",
      "Epoch 53/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.3205 - accuracy: 0.8938\n",
      "Epoch 54/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.3063 - accuracy: 0.8985\n",
      "Epoch 55/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.3090 - accuracy: 0.8956\n",
      "Epoch 56/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.3045 - accuracy: 0.8973\n",
      "Epoch 57/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.2976 - accuracy: 0.9008\n",
      "Epoch 58/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.2971 - accuracy: 0.9008\n",
      "Epoch 59/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.2977 - accuracy: 0.9009\n",
      "Epoch 60/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.2899 - accuracy: 0.9038\n",
      "Epoch 61/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.2876 - accuracy: 0.9036\n",
      "Epoch 62/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.2838 - accuracy: 0.9043\n",
      "Epoch 63/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.2769 - accuracy: 0.9070\n",
      "Epoch 64/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.2744 - accuracy: 0.9066\n",
      "Epoch 65/150\n",
      "700/700 [==============================] - 0s 588us/sample - loss: 0.2745 - accuracy: 0.9083\n",
      "Epoch 66/150\n",
      "700/700 [==============================] - 0s 598us/sample - loss: 0.2780 - accuracy: 0.9064\n",
      "Epoch 67/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.2930 - accuracy: 0.9013\n",
      "Epoch 68/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.2902 - accuracy: 0.9023\n",
      "Epoch 69/150\n",
      "700/700 [==============================] - 0s 572us/sample - loss: 0.2691 - accuracy: 0.9081\n",
      "Epoch 70/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.2589 - accuracy: 0.9119\n",
      "Epoch 71/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.2589 - accuracy: 0.9125\n",
      "Epoch 72/150\n",
      "700/700 [==============================] - 0s 593us/sample - loss: 0.2585 - accuracy: 0.9125\n",
      "Epoch 73/150\n",
      "700/700 [==============================] - 0s 592us/sample - loss: 0.2526 - accuracy: 0.9121\n",
      "Epoch 74/150\n",
      "700/700 [==============================] - 0s 580us/sample - loss: 0.2537 - accuracy: 0.9131\n",
      "Epoch 75/150\n",
      "700/700 [==============================] - 0s 591us/sample - loss: 0.2451 - accuracy: 0.9160\n",
      "Epoch 76/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.2323 - accuracy: 0.9200\n",
      "Epoch 77/150\n",
      "700/700 [==============================] - 0s 598us/sample - loss: 0.2247 - accuracy: 0.9225\n",
      "Epoch 78/150\n",
      "700/700 [==============================] - 0s 580us/sample - loss: 0.2196 - accuracy: 0.9246\n",
      "Epoch 79/150\n",
      "700/700 [==============================] - 0s 605us/sample - loss: 0.2153 - accuracy: 0.9244\n",
      "Epoch 80/150\n",
      "700/700 [==============================] - 0s 595us/sample - loss: 0.2134 - accuracy: 0.9254\n",
      "Epoch 81/150\n",
      "700/700 [==============================] - 0s 584us/sample - loss: 0.2158 - accuracy: 0.9242\n",
      "Epoch 82/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.2174 - accuracy: 0.9243\n",
      "Epoch 83/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.2118 - accuracy: 0.9257\n",
      "Epoch 84/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.2056 - accuracy: 0.9283\n",
      "Epoch 85/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.2083 - accuracy: 0.9275\n",
      "Epoch 86/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.1986 - accuracy: 0.9307\n",
      "Epoch 87/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1961 - accuracy: 0.9316 - loss: 0.1928 - accuracy\n",
      "Epoch 88/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1957 - accuracy: 0.9316\n",
      "Epoch 89/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1938 - accuracy: 0.9321\n",
      "Epoch 90/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1867 - accuracy: 0.9344\n",
      "Epoch 91/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1789 - accuracy: 0.9377\n",
      "Epoch 92/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.1768 - accuracy: 0.9382\n",
      "Epoch 93/150\n",
      "700/700 [==============================] - 0s 594us/sample - loss: 0.1772 - accuracy: 0.9363\n",
      "Epoch 94/150\n",
      "700/700 [==============================] - 0s 591us/sample - loss: 0.1791 - accuracy: 0.9370\n",
      "Epoch 95/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1724 - accuracy: 0.9393\n",
      "Epoch 96/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1655 - accuracy: 0.9414\n",
      "Epoch 97/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1664 - accuracy: 0.9415\n",
      "Epoch 98/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.1680 - accuracy: 0.9399\n",
      "Epoch 99/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1617 - accuracy: 0.9428\n",
      "Epoch 100/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.1602 - accuracy: 0.9431\n",
      "Epoch 101/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1592 - accuracy: 0.9443\n",
      "Epoch 102/150\n",
      "700/700 [==============================] - 0s 593us/sample - loss: 0.1538 - accuracy: 0.9448\n",
      "Epoch 103/150\n",
      "700/700 [==============================] - 0s 571us/sample - loss: 0.1523 - accuracy: 0.9456\n",
      "Epoch 104/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1530 - accuracy: 0.9451\n",
      "Epoch 105/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1539 - accuracy: 0.9459\n",
      "Epoch 106/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1517 - accuracy: 0.9447\n",
      "Epoch 107/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1502 - accuracy: 0.9467\n",
      "Epoch 108/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1504 - accuracy: 0.9458\n",
      "Epoch 109/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1463 - accuracy: 0.9485\n",
      "Epoch 110/150\n",
      "700/700 [==============================] - 0s 601us/sample - loss: 0.1406 - accuracy: 0.9494\n",
      "Epoch 111/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.1427 - accuracy: 0.9493\n",
      "Epoch 112/150\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.1562 - accuracy: 0.9431\n",
      "Epoch 113/150\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1503 - accuracy: 0.9466\n",
      "Epoch 114/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1485 - accuracy: 0.9465\n",
      "Epoch 115/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1460 - accuracy: 0.9474\n",
      "Epoch 116/150\n",
      "700/700 [==============================] - 0s 601us/sample - loss: 0.1398 - accuracy: 0.9490\n",
      "Epoch 117/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1353 - accuracy: 0.9517\n",
      "Epoch 118/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1331 - accuracy: 0.9522\n",
      "Epoch 119/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1393 - accuracy: 0.9493\n",
      "Epoch 120/150\n",
      "700/700 [==============================] - 0s 590us/sample - loss: 0.1378 - accuracy: 0.9498\n",
      "Epoch 121/150\n",
      "700/700 [==============================] - 0s 611us/sample - loss: 0.1430 - accuracy: 0.9487\n",
      "Epoch 122/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1329 - accuracy: 0.9523\n",
      "Epoch 123/150\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.1326 - accuracy: 0.9519\n",
      "Epoch 124/150\n",
      "700/700 [==============================] - 0s 593us/sample - loss: 0.1304 - accuracy: 0.9526\n",
      "Epoch 125/150\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.1356 - accuracy: 0.9508\n",
      "Epoch 126/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1311 - accuracy: 0.9526\n",
      "Epoch 127/150\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1290 - accuracy: 0.9540\n",
      "Epoch 128/150\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1178 - accuracy: 0.9573\n",
      "Epoch 129/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1193 - accuracy: 0.9562\n",
      "Epoch 130/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1152 - accuracy: 0.9582\n",
      "Epoch 131/150\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1187 - accuracy: 0.9573\n",
      "Epoch 132/150\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1199 - accuracy: 0.9562\n",
      "Epoch 133/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1195 - accuracy: 0.9567\n",
      "Epoch 134/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.1219 - accuracy: 0.9557\n",
      "Epoch 135/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1190 - accuracy: 0.9560\n",
      "Epoch 136/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1127 - accuracy: 0.9585\n",
      "Epoch 137/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.1183 - accuracy: 0.9570\n",
      "Epoch 138/150\n",
      "700/700 [==============================] - 0s 608us/sample - loss: 0.1148 - accuracy: 0.9584\n",
      "Epoch 139/150\n",
      "700/700 [==============================] - 0s 593us/sample - loss: 0.1148 - accuracy: 0.9590\n",
      "Epoch 140/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1118 - accuracy: 0.9592\n",
      "Epoch 141/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1088 - accuracy: 0.9607\n",
      "Epoch 142/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1032 - accuracy: 0.9621\n",
      "Epoch 143/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.0991 - accuracy: 0.9633\n",
      "Epoch 144/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1009 - accuracy: 0.9631\n",
      "Epoch 145/150\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1023 - accuracy: 0.9636\n",
      "Epoch 146/150\n",
      "700/700 [==============================] - 0s 586us/sample - loss: 0.1022 - accuracy: 0.9630\n",
      "Epoch 147/150\n",
      "700/700 [==============================] - 0s 585us/sample - loss: 0.1061 - accuracy: 0.9616\n",
      "Epoch 148/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1037 - accuracy: 0.9624\n",
      "Epoch 149/150\n",
      "700/700 [==============================] - 0s 599us/sample - loss: 0.1001 - accuracy: 0.9638\n",
      "Epoch 150/150\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1012 - accuracy: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ef83b1da48>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "ip = keras.layers.Input(shape = (75, 2048), batch_size = None)\n",
    "ip_drop = keras.layers.Dropout(0)(ip)\n",
    "ip_red1 = keras.layers.Dense(512, activation=\"relu\")(ip_drop)\n",
    "ip_red1_drop = keras.layers.Dropout(0.3)(ip_red1)\n",
    "ip_red2 = keras.layers.Dense(256, activation=\"relu\")(ip_red1_drop)\n",
    "ip_red2_drop = keras.layers.Dropout(0.2)(ip_red2)\n",
    "a1 = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(ip_red2_drop)\n",
    "a1_drop = keras.layers.Dropout(0.3)(a1)\n",
    "\n",
    "a2 = keras.layers.Dense(256, activation=\"relu\")(a1_drop)\n",
    "a2=keras.layers.Dropout(0.3)(a2)\n",
    "a3 = keras.layers.Dense(128, activation=\"relu\")(a2)\n",
    "op = keras.layers.Dense(len(vocab), activation=\"softmax\")(a3)\n",
    "\n",
    "model2=keras.models.Model(inputs=ip, outputs=op)\n",
    "model2.summary()\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(x_train,y_train, batch_size=64, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model2.predict(x_test)\n",
    "print(pred.shape)\n",
    "frame_corr = 0\n",
    "frames = 0\n",
    "for i, row in enumerate(pred):\n",
    "    for j, frame_pred in enumerate(row):\n",
    "        frames+=1\n",
    "        if np.argmax(frame_pred) == np.argmax(y_test[i, j]):\n",
    "            frame_corr += 1\n",
    "print(frame_corr, frames, frame_corr/frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 75, 2048)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 75, 2048)     0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 75, 512)      1049088     dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 75, 512)      0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 75, 256)      131328      dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 75, 256)      0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 75, 256)      394240      dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 75, 256)      0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_4 (TensorFlo [(None, 75, 512)]    0           bidirectional_4[0][0]            \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 75, 512)      0           tf_op_layer_concat_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 75, 256)      131328      dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 75, 256)      0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 75, 128)      32896       dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 75, 53)       6837        dense_23[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,745,717\n",
      "Trainable params: 1,745,717\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 700 samples\n",
      "Epoch 1/200\n",
      "700/700 [==============================] - 5s 7ms/sample - loss: 2.9940 - accuracy: 0.4419\n",
      "Epoch 2/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 2.5255 - accuracy: 0.5077\n",
      "Epoch 3/200\n",
      "700/700 [==============================] - 0s 605us/sample - loss: 2.4653 - accuracy: 0.5077\n",
      "Epoch 4/200\n",
      "700/700 [==============================] - 0s 609us/sample - loss: 2.4390 - accuracy: 0.5077\n",
      "Epoch 5/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 2.3506 - accuracy: 0.5077\n",
      "Epoch 6/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 2.1210 - accuracy: 0.5077\n",
      "Epoch 7/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 2.0399 - accuracy: 0.5081\n",
      "Epoch 8/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 1.9309 - accuracy: 0.5109\n",
      "Epoch 9/200\n",
      "700/700 [==============================] - 0s 643us/sample - loss: 1.8143 - accuracy: 0.5232\n",
      "Epoch 10/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 1.6964 - accuracy: 0.5337\n",
      "Epoch 11/200\n",
      "700/700 [==============================] - 0s 636us/sample - loss: 1.6575 - accuracy: 0.5316\n",
      "Epoch 12/200\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 1.5679 - accuracy: 0.5482\n",
      "Epoch 13/200\n",
      "700/700 [==============================] - 0s 651us/sample - loss: 1.5149 - accuracy: 0.5516\n",
      "Epoch 14/200\n",
      "700/700 [==============================] - 0s 592us/sample - loss: 1.4410 - accuracy: 0.5651\n",
      "Epoch 15/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 1.3907 - accuracy: 0.5742\n",
      "Epoch 16/200\n",
      "700/700 [==============================] - 0s 640us/sample - loss: 1.3255 - accuracy: 0.5891\n",
      "Epoch 17/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 1.2807 - accuracy: 0.6017\n",
      "Epoch 18/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 1.2253 - accuracy: 0.6213\n",
      "Epoch 19/200\n",
      "700/700 [==============================] - 0s 644us/sample - loss: 1.1442 - accuracy: 0.6450\n",
      "Epoch 20/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 1.0472 - accuracy: 0.6752\n",
      "Epoch 21/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.9611 - accuracy: 0.7002\n",
      "Epoch 22/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.8844 - accuracy: 0.7269\n",
      "Epoch 23/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.8292 - accuracy: 0.7442\n",
      "Epoch 24/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.7711 - accuracy: 0.7617\n",
      "Epoch 25/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.7413 - accuracy: 0.7695\n",
      "Epoch 26/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.7053 - accuracy: 0.7795\n",
      "Epoch 27/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.6581 - accuracy: 0.7932\n",
      "Epoch 28/200\n",
      "700/700 [==============================] - 0s 643us/sample - loss: 0.6370 - accuracy: 0.7997\n",
      "Epoch 29/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.6089 - accuracy: 0.8094\n",
      "Epoch 30/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.5695 - accuracy: 0.8209\n",
      "Epoch 31/200\n",
      "700/700 [==============================] - 0s 608us/sample - loss: 0.5533 - accuracy: 0.8235\n",
      "Epoch 32/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.5349 - accuracy: 0.8277\n",
      "Epoch 33/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.5107 - accuracy: 0.8386\n",
      "Epoch 34/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.4912 - accuracy: 0.8444\n",
      "Epoch 35/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.4772 - accuracy: 0.8483\n",
      "Epoch 36/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.4598 - accuracy: 0.8530\n",
      "Epoch 37/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.4382 - accuracy: 0.8602\n",
      "Epoch 38/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.4193 - accuracy: 0.8667\n",
      "Epoch 39/200\n",
      "700/700 [==============================] - 0s 612us/sample - loss: 0.4081 - accuracy: 0.8678\n",
      "Epoch 40/200\n",
      "700/700 [==============================] - 0s 602us/sample - loss: 0.4017 - accuracy: 0.8718\n",
      "Epoch 41/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.3961 - accuracy: 0.8730\n",
      "Epoch 42/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.3810 - accuracy: 0.8745\n",
      "Epoch 43/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.3724 - accuracy: 0.8790\n",
      "Epoch 44/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.3618 - accuracy: 0.8821 - loss: 0.3634 - accuracy: \n",
      "Epoch 45/200\n",
      "700/700 [==============================] - 0s 599us/sample - loss: 0.3446 - accuracy: 0.8875\n",
      "Epoch 46/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.3422 - accuracy: 0.8880\n",
      "Epoch 47/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.3342 - accuracy: 0.8899\n",
      "Epoch 48/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.3253 - accuracy: 0.8929\n",
      "Epoch 49/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.3237 - accuracy: 0.8920\n",
      "Epoch 50/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.3297 - accuracy: 0.8921\n",
      "Epoch 51/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.3230 - accuracy: 0.8927\n",
      "Epoch 52/200\n",
      "700/700 [==============================] - 0s 608us/sample - loss: 0.3114 - accuracy: 0.8966\n",
      "Epoch 53/200\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 0.2919 - accuracy: 0.9030\n",
      "Epoch 54/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.2882 - accuracy: 0.9033\n",
      "Epoch 55/200\n",
      "700/700 [==============================] - 0s 604us/sample - loss: 0.2840 - accuracy: 0.9043\n",
      "Epoch 56/200\n",
      "700/700 [==============================] - 0s 596us/sample - loss: 0.2782 - accuracy: 0.9059\n",
      "Epoch 57/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.2766 - accuracy: 0.9046\n",
      "Epoch 58/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.2720 - accuracy: 0.9078\n",
      "Epoch 59/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.2670 - accuracy: 0.9093\n",
      "Epoch 60/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.2625 - accuracy: 0.9106\n",
      "Epoch 61/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.2559 - accuracy: 0.9133\n",
      "Epoch 62/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.2465 - accuracy: 0.9146\n",
      "Epoch 63/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.2433 - accuracy: 0.9157\n",
      "Epoch 64/200\n",
      "700/700 [==============================] - 0s 657us/sample - loss: 0.2366 - accuracy: 0.9193\n",
      "Epoch 65/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.2357 - accuracy: 0.9188\n",
      "Epoch 66/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.2289 - accuracy: 0.9197\n",
      "Epoch 67/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.2230 - accuracy: 0.9213\n",
      "Epoch 68/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.2249 - accuracy: 0.9221\n",
      "Epoch 69/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.2187 - accuracy: 0.9234\n",
      "Epoch 70/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.2186 - accuracy: 0.9242\n",
      "Epoch 71/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.2143 - accuracy: 0.9250\n",
      "Epoch 72/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.2072 - accuracy: 0.9280\n",
      "Epoch 73/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1989 - accuracy: 0.9297\n",
      "Epoch 74/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.1906 - accuracy: 0.9333\n",
      "Epoch 75/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1899 - accuracy: 0.9333\n",
      "Epoch 76/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.1890 - accuracy: 0.9337\n",
      "Epoch 77/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1817 - accuracy: 0.9363\n",
      "Epoch 78/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1813 - accuracy: 0.9356\n",
      "Epoch 79/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1770 - accuracy: 0.9378\n",
      "Epoch 80/200\n",
      "700/700 [==============================] - 0s 643us/sample - loss: 0.1756 - accuracy: 0.9377\n",
      "Epoch 81/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1683 - accuracy: 0.9395\n",
      "Epoch 82/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.1696 - accuracy: 0.9400\n",
      "Epoch 83/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.1699 - accuracy: 0.9386\n",
      "Epoch 84/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1657 - accuracy: 0.9408\n",
      "Epoch 85/200\n",
      "700/700 [==============================] - 0s 643us/sample - loss: 0.1774 - accuracy: 0.9371\n",
      "Epoch 86/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.1722 - accuracy: 0.9386\n",
      "Epoch 87/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1657 - accuracy: 0.9411\n",
      "Epoch 88/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1546 - accuracy: 0.9443\n",
      "Epoch 89/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.1568 - accuracy: 0.9447\n",
      "Epoch 90/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1824 - accuracy: 0.9352\n",
      "Epoch 91/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1694 - accuracy: 0.9387\n",
      "Epoch 92/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1574 - accuracy: 0.9435\n",
      "Epoch 93/200\n",
      "700/700 [==============================] - 0s 643us/sample - loss: 0.1512 - accuracy: 0.9458\n",
      "Epoch 94/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.1476 - accuracy: 0.9469\n",
      "Epoch 95/200\n",
      "700/700 [==============================] - 0s 643us/sample - loss: 0.1398 - accuracy: 0.9498\n",
      "Epoch 96/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.1399 - accuracy: 0.9501\n",
      "Epoch 97/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.1402 - accuracy: 0.9494\n",
      "Epoch 98/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1388 - accuracy: 0.9488\n",
      "Epoch 99/200\n",
      "700/700 [==============================] - 0s 622us/sample - loss: 0.1306 - accuracy: 0.9520\n",
      "Epoch 100/200\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 0.1341 - accuracy: 0.9522\n",
      "Epoch 101/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1337 - accuracy: 0.9515\n",
      "Epoch 102/200\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 0.1298 - accuracy: 0.9525\n",
      "Epoch 103/200\n",
      "700/700 [==============================] - 0s 608us/sample - loss: 0.1279 - accuracy: 0.9547\n",
      "Epoch 104/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.1269 - accuracy: 0.9542\n",
      "Epoch 105/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.1232 - accuracy: 0.9539\n",
      "Epoch 106/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1203 - accuracy: 0.9557\n",
      "Epoch 107/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1186 - accuracy: 0.9550\n",
      "Epoch 108/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1223 - accuracy: 0.9558\n",
      "Epoch 109/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1179 - accuracy: 0.9574\n",
      "Epoch 110/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1239 - accuracy: 0.9550\n",
      "Epoch 111/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1196 - accuracy: 0.9565\n",
      "Epoch 112/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1182 - accuracy: 0.9578\n",
      "Epoch 113/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1159 - accuracy: 0.9566\n",
      "Epoch 114/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1199 - accuracy: 0.9565\n",
      "Epoch 115/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.1171 - accuracy: 0.9572\n",
      "Epoch 116/200\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 0.1190 - accuracy: 0.9570\n",
      "Epoch 117/200\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 0.1118 - accuracy: 0.9585\n",
      "Epoch 118/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1092 - accuracy: 0.9599\n",
      "Epoch 119/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1034 - accuracy: 0.9629\n",
      "Epoch 120/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1029 - accuracy: 0.9630\n",
      "Epoch 121/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0980 - accuracy: 0.9636\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 601us/sample - loss: 0.0996 - accuracy: 0.9641\n",
      "Epoch 123/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0982 - accuracy: 0.9637\n",
      "Epoch 124/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1024 - accuracy: 0.9624\n",
      "Epoch 125/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1031 - accuracy: 0.9626\n",
      "Epoch 126/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1048 - accuracy: 0.9617\n",
      "Epoch 127/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1021 - accuracy: 0.9620\n",
      "Epoch 128/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0961 - accuracy: 0.9646\n",
      "Epoch 129/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1018 - accuracy: 0.9633\n",
      "Epoch 130/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.1019 - accuracy: 0.9626\n",
      "Epoch 131/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0964 - accuracy: 0.9645\n",
      "Epoch 132/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0995 - accuracy: 0.9634\n",
      "Epoch 133/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.0985 - accuracy: 0.9645\n",
      "Epoch 134/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0884 - accuracy: 0.9674\n",
      "Epoch 135/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0899 - accuracy: 0.9670\n",
      "Epoch 136/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0901 - accuracy: 0.9666\n",
      "Epoch 137/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0909 - accuracy: 0.9666\n",
      "Epoch 138/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0891 - accuracy: 0.9679\n",
      "Epoch 139/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0934 - accuracy: 0.9662\n",
      "Epoch 140/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.0955 - accuracy: 0.9650\n",
      "Epoch 141/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.1005 - accuracy: 0.9628\n",
      "Epoch 142/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0936 - accuracy: 0.9654\n",
      "Epoch 143/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.0877 - accuracy: 0.9678\n",
      "Epoch 144/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0831 - accuracy: 0.9699\n",
      "Epoch 145/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0802 - accuracy: 0.9698\n",
      "Epoch 146/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0829 - accuracy: 0.9692\n",
      "Epoch 147/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0813 - accuracy: 0.9699\n",
      "Epoch 148/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.0864 - accuracy: 0.9677\n",
      "Epoch 149/200\n",
      "700/700 [==============================] - 0s 592us/sample - loss: 0.0807 - accuracy: 0.9700\n",
      "Epoch 150/200\n",
      "700/700 [==============================] - 0s 608us/sample - loss: 0.0855 - accuracy: 0.9697\n",
      "Epoch 151/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0780 - accuracy: 0.9712\n",
      "Epoch 152/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0849 - accuracy: 0.9683\n",
      "Epoch 153/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0789 - accuracy: 0.9717\n",
      "Epoch 154/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0841 - accuracy: 0.9692\n",
      "Epoch 155/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0823 - accuracy: 0.9698\n",
      "Epoch 156/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0771 - accuracy: 0.9720\n",
      "Epoch 157/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0773 - accuracy: 0.9716\n",
      "Epoch 158/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0768 - accuracy: 0.9717\n",
      "Epoch 159/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0741 - accuracy: 0.9730\n",
      "Epoch 160/200\n",
      "700/700 [==============================] - 0s 599us/sample - loss: 0.0760 - accuracy: 0.9726\n",
      "Epoch 161/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0812 - accuracy: 0.9708\n",
      "Epoch 162/200\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 0.0748 - accuracy: 0.9730\n",
      "Epoch 163/200\n",
      "700/700 [==============================] - 0s 601us/sample - loss: 0.0726 - accuracy: 0.9735\n",
      "Epoch 164/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0746 - accuracy: 0.9735\n",
      "Epoch 165/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0787 - accuracy: 0.9713\n",
      "Epoch 166/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0860 - accuracy: 0.9682\n",
      "Epoch 167/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0849 - accuracy: 0.9688\n",
      "Epoch 168/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0800 - accuracy: 0.9705\n",
      "Epoch 169/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.0741 - accuracy: 0.9731\n",
      "Epoch 170/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0671 - accuracy: 0.9759\n",
      "Epoch 171/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.0627 - accuracy: 0.9775\n",
      "Epoch 172/200\n",
      "700/700 [==============================] - 0s 649us/sample - loss: 0.0628 - accuracy: 0.9765\n",
      "Epoch 173/200\n",
      "700/700 [==============================] - 0s 650us/sample - loss: 0.0641 - accuracy: 0.9764\n",
      "Epoch 174/200\n",
      "700/700 [==============================] - 0s 628us/sample - loss: 0.0637 - accuracy: 0.9769\n",
      "Epoch 175/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.0629 - accuracy: 0.9769\n",
      "Epoch 176/200\n",
      "700/700 [==============================] - 0s 616us/sample - loss: 0.0620 - accuracy: 0.9767\n",
      "Epoch 177/200\n",
      "700/700 [==============================] - 0s 618us/sample - loss: 0.0640 - accuracy: 0.9764\n",
      "Epoch 178/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.0626 - accuracy: 0.9765\n",
      "Epoch 179/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0650 - accuracy: 0.9763\n",
      "Epoch 180/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0646 - accuracy: 0.9760\n",
      "Epoch 181/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0672 - accuracy: 0.9748\n",
      "Epoch 182/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0705 - accuracy: 0.9744\n",
      "Epoch 183/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.0641 - accuracy: 0.9767\n",
      "Epoch 184/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0667 - accuracy: 0.9757\n",
      "Epoch 185/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0752 - accuracy: 0.9730\n",
      "Epoch 186/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0715 - accuracy: 0.9743\n",
      "Epoch 187/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0658 - accuracy: 0.9757\n",
      "Epoch 188/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.0625 - accuracy: 0.9771\n",
      "Epoch 189/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0585 - accuracy: 0.9790\n",
      "Epoch 190/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.0607 - accuracy: 0.9775\n",
      "Epoch 191/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0650 - accuracy: 0.9765\n",
      "Epoch 192/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0622 - accuracy: 0.9765\n",
      "Epoch 193/200\n",
      "700/700 [==============================] - 0s 607us/sample - loss: 0.0611 - accuracy: 0.9785\n",
      "Epoch 194/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.0602 - accuracy: 0.9776\n",
      "Epoch 195/200\n",
      "700/700 [==============================] - 0s 629us/sample - loss: 0.0573 - accuracy: 0.9793\n",
      "Epoch 196/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0537 - accuracy: 0.9803\n",
      "Epoch 197/200\n",
      "700/700 [==============================] - 0s 614us/sample - loss: 0.0558 - accuracy: 0.9795\n",
      "Epoch 198/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0555 - accuracy: 0.9796\n",
      "Epoch 199/200\n",
      "700/700 [==============================] - 0s 600us/sample - loss: 0.0550 - accuracy: 0.9801\n",
      "Epoch 200/200\n",
      "700/700 [==============================] - 0s 615us/sample - loss: 0.0549 - accuracy: 0.9801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ef25cf4ac8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "ip = keras.layers.Input(shape = (75, 2048), batch_size = None)\n",
    "ip_drop = keras.layers.Dropout(0)(ip)\n",
    "ip_red1 = keras.layers.Dense(256*2, activation=\"relu\")(ip_drop)\n",
    "ip_red1_drop = keras.layers.Dropout(0.3)(ip_red1)\n",
    "ip_red2 = keras.layers.Dense(128*2, activation=\"relu\")(ip_red1_drop)\n",
    "ip_red2_drop = keras.layers.Dropout(0.2)(ip_red2)\n",
    "bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(64*2, return_sequences=True))\n",
    "a1 = bi_lstm(ip_red2_drop)\n",
    "\n",
    "t_model1 = keras.models.Model(inputs=ip, outputs=a1)\n",
    "\n",
    "attn_res = keras.layers.Attention()([a1, a1])\n",
    "\n",
    "t_model2 = keras.models.Model(inputs=ip, outputs=attn_res)\n",
    "\n",
    "res = tf.concat([a1, attn_res], axis = -1)\n",
    "res = keras.layers.Dropout(0.3)(res)\n",
    "\n",
    "# x = keras.layers.Dense(512, activation=\"relu\")(res)\n",
    "# x=keras.layers.Dropout(0.2)(x)\n",
    "a2 = keras.layers.Dense(256, activation=\"relu\")(res)\n",
    "a2=keras.layers.Dropout(0.3)(a2)\n",
    "a3 = keras.layers.Dense(128, activation=\"relu\")(a2)\n",
    "op = keras.layers.Dense(len(vocab), activation=\"softmax\")(a3)\n",
    "\n",
    "model3=keras.models.Model(inputs=ip, outputs=op)\n",
    "model3.summary()\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.fit(x_train,y_train, batch_size=64, epochs=200)\n",
    "# print(t_model1.summary(),end=\"\\n--------------\\n\")\n",
    "# print(t_model2.summary(),end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "def get_sent_from_preds(labels): #(bs, 75, 53)\n",
    "    labels = np.argmax(labels, axis=2) #(bs, 75)\n",
    "    res = []\n",
    "    for vid_label in labels:\n",
    "        vid_res = [\"sil\"]\n",
    "        for word_index in vid_label:\n",
    "            word = vocab[word_index]\n",
    "            if not vid_res[-1] == word:\n",
    "                vid_res.append(word)\n",
    "        res.append(\" \".join(vid_res).strip())\n",
    "    return res\n",
    "\n",
    "def get_wer(model, x, y):\n",
    "    return wer(get_sent_from_preds(y), get_sent_from_preds(model.predict(x)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model4.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13159528243327126"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wer(model4, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sil set blue at g nine soon sil',\n",
       "  'sil bin green at n four now sil',\n",
       "  'sil place blue by p five again sil',\n",
       "  'sil set green at c three soon sil',\n",
       "  'sil place red by d three again sil',\n",
       "  'sil place white with y three soon sil',\n",
       "  'sil place green in d six please sil',\n",
       "  'sil bin green in n three again sil',\n",
       "  'sil set green in b nine soon sil',\n",
       "  'sil lay white at r eight please sil'],\n",
       " ['sil set sil blue at u g zero nine zero soon two soon sil',\n",
       "  'sil place blue green in at f five four now sil',\n",
       "  'sil place blue by p v five v five now again sil',\n",
       "  'sil set green in c three soon four soon sil',\n",
       "  'sil place green red by k d seven three again sil',\n",
       "  'sil place green red white red green with in three soon three soon sil',\n",
       "  'sil set place green in z c seven six seven please six please sil',\n",
       "  'sil bin green in a n i five three again sil',\n",
       "  'sil set green in green by p in at by y nine in zero soon sil',\n",
       "  'sil lay white at i h eight five please seven six now sil'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 10\n",
    "get_sent_from_preds(y_test[:x]), get_sent_from_preds(model1.predict(x_test[:x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.02780863e-02 1.98178414e-02 3.22985754e-02 ... 8.68121407e-07\n",
      "  4.43965412e-07 1.82224390e-07]\n",
      " [2.54249751e-03 6.79539312e-03 1.47261271e-02 ... 1.74323657e-07\n",
      "  9.30165571e-08 4.00493831e-08]\n",
      " [5.20501939e-04 1.84979389e-03 5.27676874e-03 ... 3.25707460e-08\n",
      "  1.82970889e-08 8.34355113e-09]\n",
      " ...\n",
      " [3.75873646e-05 5.88319915e-05 8.75084784e-05 ... 6.05027666e-02\n",
      "  4.11833911e-02 2.38610683e-02]\n",
      " [3.52122325e-05 5.75043233e-05 9.00507716e-05 ... 7.54405974e-02\n",
      "  5.84238179e-02 3.96820538e-02]\n",
      " [3.00871568e-05 5.15425957e-05 8.54843457e-05 ... 9.09919804e-02\n",
      "  8.26084285e-02 6.91426213e-02]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f1abd80448>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "# model3.evaluate(x_test, y_test)\n",
    "sample = np.expand_dims(x_test[16], axis=0)\n",
    "\n",
    "# print(t_model1.predict(sample).shape)\n",
    "t1_preds = np.squeeze(t_model3.predict(sample))\n",
    "t1_preds.shape\n",
    "\n",
    "t2_preds = np.squeeze(t_model4.predict(sample))\n",
    "\n",
    "scores = np.zeros((75, 75))\n",
    "for i, frame_pred in enumerate(t1_preds):\n",
    "    attn_res = np.zeros((256))\n",
    "    for j, other_frame_pred in enumerate(t1_preds):\n",
    "        scores[i, j] = np.sum(frame_pred * other_frame_pred)\n",
    "    scores[i] = softmax(scores[i])\n",
    "    for j, other_frame_pred in enumerate(t1_preds):\n",
    "        attn_res += scores[i, j] * other_frame_pred\n",
    "#     print(t2_preds[i])\n",
    "#     print(t2_preds[i] - attn_res)\n",
    "#     break\n",
    "    \n",
    "    \n",
    "# print(t_model2.predict(sample).shape)\n",
    "print(scores)\n",
    "sns.heatmap(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "def condense(preds):\n",
    "    global vocab\n",
    "    last_pred = -1\n",
    "    last_pred_freq = 0\n",
    "    fin_preds = []\n",
    "    for pred in preds:\n",
    "        if not pred == last_pred:\n",
    "            if not len(fin_preds) == 0:\n",
    "                fin_preds[len(fin_preds) - 1][1] = last_pred_freq + 1\n",
    "            last_pred_freq = 0\n",
    "            fin_preds.append([pred, 0])\n",
    "            last_pred = pred\n",
    "        else:\n",
    "            last_pred_freq += 1 \n",
    "    s=\"\"\n",
    "    for pred, freq in fin_preds:\n",
    "        s += \" \" + vocab[pred]\n",
    "    s = s.strip()    \n",
    "    return fin_preds, s\n",
    "\n",
    "ground_truth = []\n",
    "hypo = []\n",
    "all_preds = model4.predict(x_test)\n",
    "for sample_pred, label in zip(all_preds, y_test):\n",
    "#     sample = np.expand_dims(sample, axis=0)\n",
    "#     preds = np.squeeze(model3.predict(sample))\n",
    "    preds = np.argmax(sample_pred, axis=1)\n",
    "    labels = np.argmax(label, axis=1)\n",
    "    \n",
    "    _, s = condense(preds)\n",
    "    hypo.append(\" \".join(list(s)))\n",
    "    _, s = condense(labels)\n",
    "    ground_truth.append(\" \".join(list(s)))\n",
    "    \n",
    "print(wer(ground_truth, hypo))\n",
    "#     print(preds,\"\\n\", condense(preds))\n",
    "#     print(labels,\"\\n\", condense(labels))\n",
    "    \n",
    "#     print(\"\\n----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "ip = keras.layers.Input(shape = (75, 2048), batch_size = None)\n",
    "ip_drop = keras.layers.Dropout(0)(ip)\n",
    "ip_red1 = keras.layers.Dense(512, activation=\"relu\")(ip_drop)\n",
    "ip_red1_drop = keras.layers.Dropout(0.3)(ip_red1)\n",
    "ip_red2 = keras.layers.Dense(256, activation=\"relu\")(ip_red1_drop)\n",
    "ip_red2_drop = keras.layers.Dropout(0.2)(ip_red2)\n",
    "bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(128*2, return_sequences=True))\n",
    "a1 = bi_lstm(ip_red2_drop)\n",
    "\n",
    "# t_model1 = keras.models.Model(inputs=ip, outputs=a1)\n",
    "\n",
    "# attn_res = keras.layers.Attention()([a1, a1])\n",
    "\n",
    "# t_model2 = keras.models.Model(inputs=ip, outputs=attn_res)\n",
    "\n",
    "# res = tf.concat([a1, attn_res], axis = -1)\n",
    "a1 = keras.layers.Dropout(0.3)(a1)\n",
    "\n",
    "# x = keras.layers.Dense(512, activation=\"relu\")(res)\n",
    "# x=keras.layers.Dropout(0.2)(x)\n",
    "a2 = keras.layers.Dense(256, activation=\"relu\")(a1)\n",
    "a2=keras.layers.Dropout(0.1)(a2)\n",
    "a3 = keras.layers.Dense(128, activation=\"relu\")(a2)\n",
    "\n",
    "t_model1 = keras.models.Model(inputs=ip, outputs=a3)\n",
    "\n",
    "attn_res = keras.layers.Attention()([a3, a3])\n",
    "\n",
    "t_model2 = keras.models.Model(inputs=ip, outputs=attn_res)\n",
    "\n",
    "res = tf.concat([a3, attn_res], axis = -1)\n",
    "\n",
    "a4= keras.layers.Dense(64, activation=\"relu\")(res)\n",
    "op = keras.layers.Dense(len(vocab), activation=\"softmax\")(a4)\n",
    "\n",
    "model4=keras.models.Model(inputs=ip, outputs=op)\n",
    "model4.summary()\n",
    "model4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model4.fit(x_train,y_train, batch_size=64, epochs=350)\n",
    "# print(t_model1.summary(),end=\"\\n--------------\\n\")\n",
    "# print(t_model2.summary(),end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>initialise with lang models weights and make it non trainable</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_118\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_30 (InputLayer)           [(None, 75, 2048)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_130 (Dropout)           (None, 75, 2048)     0           input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_190 (Dense)               (None, 75, 512)      1049088     dropout_130[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_131 (Dropout)           (None, 75, 512)      0           dense_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_191 (Dense)               (None, 75, 256)      131328      dropout_131[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_132 (Dropout)           (None, 75, 256)      0           dense_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_38 (Bidirectional (None, 75, 256)      394240      dropout_132[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_43 (Attention)        (None, 75, 256)      0           bidirectional_38[0][0]           \n",
      "                                                                 bidirectional_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_51 (TensorFl [(None, 75, 512)]    0           bidirectional_38[0][0]           \n",
      "                                                                 attention_43[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_133 (Dropout)           (None, 75, 512)      0           tf_op_layer_concat_51[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_192 (Dense)               (None, 75, 256)      131328      dropout_133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_134 (Dropout)           (None, 75, 256)      0           dense_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_193 (Dense)               (None, 75, 128)      32896       dropout_134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_194 (Dense)               (None, 75, 53)       6837        dense_193[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_195 (Dense)               (None, 75, 128)      6912        dense_194[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_39 (Bidirectional (None, 75, 256)      263168      dense_195[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_44 (Attention)        (None, 75, 256)      0           bidirectional_39[0][0]           \n",
      "                                                                 bidirectional_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_52 (TensorFl [(None, 75, 512)]    0           bidirectional_39[0][0]           \n",
      "                                                                 attention_44[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_196 (Dense)               (None, 75, 256)      131328      tf_op_layer_concat_52[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_197 (Dense)               (None, 75, 128)      32896       dense_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_198 (Dense)               (None, 75, 53)       6837        dense_197[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,186,858\n",
      "Trainable params: 2,186,858\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 700 samples\n",
      "Epoch 1/210\n",
      "700/700 [==============================] - 8s 11ms/sample - loss: 8.3919 - dense_194_loss: 3.0955 - dense_198_loss: 3.4171 - dense_194_accuracy: 0.4269 - dense_198_accuracy: 0.4619\n",
      "Epoch 2/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 6.5321 - dense_194_loss: 2.5839 - dense_198_loss: 2.6193 - dense_194_accuracy: 0.5077 - dense_198_accuracy: 0.5077\n",
      "Epoch 3/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 6.1827 - dense_194_loss: 2.4963 - dense_198_loss: 2.4672 - dense_194_accuracy: 0.5077 - dense_198_accuracy: 0.5077\n",
      "Epoch 4/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 5.9553 - dense_194_loss: 2.4843 - dense_198_loss: 2.3564 - dense_194_accuracy: 0.5077 - dense_198_accuracy: 0.5077\n",
      "Epoch 5/210\n",
      "700/700 [==============================] - 1s 904us/sample - loss: 5.3762 - dense_194_loss: 2.4668 - dense_198_loss: 2.0705 - dense_194_accuracy: 0.5077 - dense_198_accuracy: 0.5079\n",
      "Epoch 6/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 4.9269 - dense_194_loss: 2.4616 - dense_198_loss: 1.8475 - dense_194_accuracy: 0.5077 - dense_198_accuracy: 0.5157\n",
      "Epoch 7/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 4.6942 - dense_194_loss: 2.4466 - dense_198_loss: 1.7350 - dense_194_accuracy: 0.5077 - dense_198_accuracy: 0.5264\n",
      "Epoch 8/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 4.4732 - dense_194_loss: 2.4142 - dense_198_loss: 1.6328 - dense_194_accuracy: 0.5077 - dense_198_accuracy: 0.5309\n",
      "Epoch 9/210\n",
      "700/700 [==============================] - 1s 891us/sample - loss: 4.2196 - dense_194_loss: 2.2554 - dense_198_loss: 1.5462 - dense_194_accuracy: 0.5077 - dense_198_accuracy: 0.5408\n",
      "Epoch 10/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 4.0566 - dense_194_loss: 2.1027 - dense_198_loss: 1.5024 - dense_194_accuracy: 0.5077 - dense_198_accuracy: 0.5426\n",
      "Epoch 11/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 3.8317 - dense_194_loss: 2.0174 - dense_198_loss: 1.4116 - dense_194_accuracy: 0.5094 - dense_198_accuracy: 0.5553\n",
      "Epoch 12/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 3.7759 - dense_194_loss: 1.9716 - dense_198_loss: 1.3948 - dense_194_accuracy: 0.5137 - dense_198_accuracy: 0.5544\n",
      "Epoch 13/210\n",
      "700/700 [==============================] - 1s 904us/sample - loss: 3.6505 - dense_194_loss: 1.9151 - dense_198_loss: 1.3467 - dense_194_accuracy: 0.5193 - dense_198_accuracy: 0.5636\n",
      "Epoch 14/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 3.5855 - dense_194_loss: 1.8763 - dense_198_loss: 1.3241 - dense_194_accuracy: 0.5210 - dense_198_accuracy: 0.5655\n",
      "Epoch 15/210\n",
      "700/700 [==============================] - 1s 869us/sample - loss: 3.5210 - dense_194_loss: 1.7814 - dense_198_loss: 1.3149 - dense_194_accuracy: 0.5314 - dense_198_accuracy: 0.5654\n",
      "Epoch 16/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 3.3688 - dense_194_loss: 1.6681 - dense_198_loss: 1.2673 - dense_194_accuracy: 0.5440 - dense_198_accuracy: 0.5707\n",
      "Epoch 17/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 3.2655 - dense_194_loss: 1.5859 - dense_198_loss: 1.2363 - dense_194_accuracy: 0.5485 - dense_198_accuracy: 0.5736\n",
      "Epoch 18/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 3.2272 - dense_194_loss: 1.5271 - dense_198_loss: 1.2319 - dense_194_accuracy: 0.5563 - dense_198_accuracy: 0.5746\n",
      "Epoch 19/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 3.2214 - dense_194_loss: 1.5079 - dense_198_loss: 1.2337 - dense_194_accuracy: 0.5567 - dense_198_accuracy: 0.5759\n",
      "Epoch 20/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 3.1479 - dense_194_loss: 1.4533 - dense_198_loss: 1.2103 - dense_194_accuracy: 0.5679 - dense_198_accuracy: 0.5785\n",
      "Epoch 21/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 3.0813 - dense_194_loss: 1.3877 - dense_198_loss: 1.1937 - dense_194_accuracy: 0.5783 - dense_198_accuracy: 0.5822\n",
      "Epoch 22/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 3.0458 - dense_194_loss: 1.3570 - dense_198_loss: 1.1837 - dense_194_accuracy: 0.5843 - dense_198_accuracy: 0.5834\n",
      "Epoch 23/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 3.1402 - dense_194_loss: 1.4096 - dense_198_loss: 1.2178 - dense_194_accuracy: 0.5752 - dense_198_accuracy: 0.5802\n",
      "Epoch 24/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 3.0488 - dense_194_loss: 1.3343 - dense_198_loss: 1.1908 - dense_194_accuracy: 0.5919 - dense_198_accuracy: 0.5832\n",
      "Epoch 25/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 2.9704 - dense_194_loss: 1.2821 - dense_198_loss: 1.1646 - dense_194_accuracy: 0.6021 - dense_198_accuracy: 0.5890\n",
      "Epoch 26/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 2.9553 - dense_194_loss: 1.2352 - dense_198_loss: 1.1689 - dense_194_accuracy: 0.6094 - dense_198_accuracy: 0.5953\n",
      "Epoch 27/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 2.8828 - dense_194_loss: 1.1927 - dense_198_loss: 1.1432 - dense_194_accuracy: 0.6252 - dense_198_accuracy: 0.5976\n",
      "Epoch 28/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 2.8123 - dense_194_loss: 1.1361 - dense_198_loss: 1.1223 - dense_194_accuracy: 0.6433 - dense_198_accuracy: 0.6076\n",
      "Epoch 29/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 2.7946 - dense_194_loss: 1.1167 - dense_198_loss: 1.1180 - dense_194_accuracy: 0.6486 - dense_198_accuracy: 0.6084\n",
      "Epoch 30/210\n",
      "700/700 [==============================] - 1s 837us/sample - loss: 2.7356 - dense_194_loss: 1.0781 - dense_198_loss: 1.0981 - dense_194_accuracy: 0.6627 - dense_198_accuracy: 0.6142\n",
      "Epoch 31/210\n",
      "700/700 [==============================] - 1s 837us/sample - loss: 2.6945 - dense_194_loss: 1.0659 - dense_198_loss: 1.0806 - dense_194_accuracy: 0.6681 - dense_198_accuracy: 0.6242\n",
      "Epoch 32/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 2.6013 - dense_194_loss: 1.0071 - dense_198_loss: 1.0489 - dense_194_accuracy: 0.6857 - dense_198_accuracy: 0.6400\n",
      "Epoch 33/210\n",
      "700/700 [==============================] - 1s 904us/sample - loss: 2.5048 - dense_194_loss: 0.9624 - dense_198_loss: 1.0116 - dense_194_accuracy: 0.6982 - dense_198_accuracy: 0.6606\n",
      "Epoch 34/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 2.4007 - dense_194_loss: 0.9271 - dense_198_loss: 0.9684 - dense_194_accuracy: 0.7102 - dense_198_accuracy: 0.6723\n",
      "Epoch 35/210\n",
      "700/700 [==============================] - 1s 891us/sample - loss: 2.2950 - dense_194_loss: 0.8864 - dense_198_loss: 0.9256 - dense_194_accuracy: 0.7222 - dense_198_accuracy: 0.6855\n",
      "Epoch 36/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 2.2472 - dense_194_loss: 0.8734 - dense_198_loss: 0.9056 - dense_194_accuracy: 0.7288 - dense_198_accuracy: 0.6955\n",
      "Epoch 37/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 2.2568 - dense_194_loss: 0.8742 - dense_198_loss: 0.9097 - dense_194_accuracy: 0.7322 - dense_198_accuracy: 0.6962\n",
      "Epoch 38/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 2.1717 - dense_194_loss: 0.8599 - dense_198_loss: 0.8706 - dense_194_accuracy: 0.7345 - dense_198_accuracy: 0.7070\n",
      "Epoch 39/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 2.0773 - dense_194_loss: 0.8129 - dense_198_loss: 0.8353 - dense_194_accuracy: 0.7509 - dense_198_accuracy: 0.7117\n",
      "Epoch 40/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 2.0292 - dense_194_loss: 0.7903 - dense_198_loss: 0.8168 - dense_194_accuracy: 0.7561 - dense_198_accuracy: 0.7225\n",
      "Epoch 41/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.9486 - dense_194_loss: 0.7600 - dense_198_loss: 0.7841 - dense_194_accuracy: 0.7683 - dense_198_accuracy: 0.7401\n",
      "Epoch 42/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.8288 - dense_194_loss: 0.7234 - dense_198_loss: 0.7331 - dense_194_accuracy: 0.7769 - dense_198_accuracy: 0.7692\n",
      "Epoch 43/210\n",
      "700/700 [==============================] - 1s 891us/sample - loss: 1.7284 - dense_194_loss: 0.6906 - dense_198_loss: 0.6914 - dense_194_accuracy: 0.7896 - dense_198_accuracy: 0.7814\n",
      "Epoch 44/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 1.6728 - dense_194_loss: 0.6966 - dense_198_loss: 0.6621 - dense_194_accuracy: 0.7873 - dense_198_accuracy: 0.7945\n",
      "Epoch 45/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.5949 - dense_194_loss: 0.6747 - dense_198_loss: 0.6288 - dense_194_accuracy: 0.7926 - dense_198_accuracy: 0.8051\n",
      "Epoch 46/210\n",
      "700/700 [==============================] - 1s 904us/sample - loss: 1.5258 - dense_194_loss: 0.6500 - dense_198_loss: 0.6003 - dense_194_accuracy: 0.8014 - dense_198_accuracy: 0.8125\n",
      "Epoch 47/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.4941 - dense_194_loss: 0.6403 - dense_198_loss: 0.5870 - dense_194_accuracy: 0.8035 - dense_198_accuracy: 0.8186\n",
      "Epoch 48/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.4696 - dense_194_loss: 0.6338 - dense_198_loss: 0.5764 - dense_194_accuracy: 0.8049 - dense_198_accuracy: 0.8184\n",
      "Epoch 49/210\n",
      "700/700 [==============================] - 1s 869us/sample - loss: 1.5480 - dense_194_loss: 0.6651 - dense_198_loss: 0.6078 - dense_194_accuracy: 0.7987 - dense_198_accuracy: 0.8120\n",
      "Epoch 50/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.4558 - dense_194_loss: 0.6359 - dense_198_loss: 0.5686 - dense_194_accuracy: 0.8058 - dense_198_accuracy: 0.8218\n",
      "Epoch 51/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.3725 - dense_194_loss: 0.6043 - dense_198_loss: 0.5352 - dense_194_accuracy: 0.8145 - dense_198_accuracy: 0.8332\n",
      "Epoch 52/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.3632 - dense_194_loss: 0.5940 - dense_198_loss: 0.5330 - dense_194_accuracy: 0.8202 - dense_198_accuracy: 0.8313\n",
      "Epoch 53/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.3065 - dense_194_loss: 0.5761 - dense_198_loss: 0.5094 - dense_194_accuracy: 0.8240 - dense_198_accuracy: 0.8392\n",
      "Epoch 54/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.2834 - dense_194_loss: 0.5691 - dense_198_loss: 0.4995 - dense_194_accuracy: 0.8259 - dense_198_accuracy: 0.8428\n",
      "Epoch 55/210\n",
      "700/700 [==============================] - 1s 891us/sample - loss: 1.2637 - dense_194_loss: 0.5580 - dense_198_loss: 0.4922 - dense_194_accuracy: 0.8277 - dense_198_accuracy: 0.8435\n",
      "Epoch 56/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 1.2372 - dense_194_loss: 0.5502 - dense_198_loss: 0.4810 - dense_194_accuracy: 0.8304 - dense_198_accuracy: 0.8476\n",
      "Epoch 57/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 1.2018 - dense_194_loss: 0.5395 - dense_198_loss: 0.4659 - dense_194_accuracy: 0.8345 - dense_198_accuracy: 0.8491\n",
      "Epoch 58/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 1.1565 - dense_194_loss: 0.5247 - dense_198_loss: 0.4470 - dense_194_accuracy: 0.8397 - dense_198_accuracy: 0.8569\n",
      "Epoch 59/210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 1s 882us/sample - loss: 1.2173 - dense_194_loss: 0.5478 - dense_198_loss: 0.4717 - dense_194_accuracy: 0.8302 - dense_198_accuracy: 0.8508\n",
      "Epoch 60/210\n",
      "700/700 [==============================] - 1s 837us/sample - loss: 1.1592 - dense_194_loss: 0.5324 - dense_198_loss: 0.4465 - dense_194_accuracy: 0.8371 - dense_198_accuracy: 0.8574\n",
      "Epoch 61/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 1.1539 - dense_194_loss: 0.5336 - dense_198_loss: 0.4435 - dense_194_accuracy: 0.8351 - dense_198_accuracy: 0.8582\n",
      "Epoch 62/210\n",
      "700/700 [==============================] - 1s 904us/sample - loss: 1.1273 - dense_194_loss: 0.5219 - dense_198_loss: 0.4331 - dense_194_accuracy: 0.8400 - dense_198_accuracy: 0.8607\n",
      "Epoch 63/210\n",
      "700/700 [==============================] - 1s 926us/sample - loss: 1.0973 - dense_194_loss: 0.5140 - dense_198_loss: 0.4202 - dense_194_accuracy: 0.8431 - dense_198_accuracy: 0.8650\n",
      "Epoch 64/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 1.0519 - dense_194_loss: 0.4960 - dense_198_loss: 0.4019 - dense_194_accuracy: 0.8492 - dense_198_accuracy: 0.8709\n",
      "Epoch 65/210\n",
      "700/700 [==============================] - 1s 913us/sample - loss: 1.0179 - dense_194_loss: 0.4826 - dense_198_loss: 0.3883 - dense_194_accuracy: 0.8520 - dense_198_accuracy: 0.8738\n",
      "Epoch 66/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.9769 - dense_194_loss: 0.4722 - dense_198_loss: 0.3704 - dense_194_accuracy: 0.8557 - dense_198_accuracy: 0.8784\n",
      "Epoch 67/210\n",
      "700/700 [==============================] - 1s 904us/sample - loss: 0.9403 - dense_194_loss: 0.4599 - dense_198_loss: 0.3551 - dense_194_accuracy: 0.8594 - dense_198_accuracy: 0.8826\n",
      "Epoch 68/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.9233 - dense_194_loss: 0.4471 - dense_198_loss: 0.3500 - dense_194_accuracy: 0.8637 - dense_198_accuracy: 0.8843\n",
      "Epoch 69/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.8821 - dense_194_loss: 0.4366 - dense_198_loss: 0.3318 - dense_194_accuracy: 0.8650 - dense_198_accuracy: 0.8890\n",
      "Epoch 70/210\n",
      "700/700 [==============================] - 1s 891us/sample - loss: 0.8538 - dense_194_loss: 0.4301 - dense_198_loss: 0.3194 - dense_194_accuracy: 0.8695 - dense_198_accuracy: 0.8944\n",
      "Epoch 71/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.8560 - dense_194_loss: 0.4308 - dense_198_loss: 0.3201 - dense_194_accuracy: 0.8674 - dense_198_accuracy: 0.8913\n",
      "Epoch 72/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.8260 - dense_194_loss: 0.4233 - dense_198_loss: 0.3072 - dense_194_accuracy: 0.8696 - dense_198_accuracy: 0.8961\n",
      "Epoch 73/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.8307 - dense_194_loss: 0.4198 - dense_198_loss: 0.3104 - dense_194_accuracy: 0.8692 - dense_198_accuracy: 0.8948\n",
      "Epoch 74/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.8795 - dense_194_loss: 0.4383 - dense_198_loss: 0.3301 - dense_194_accuracy: 0.8638 - dense_198_accuracy: 0.8888\n",
      "Epoch 75/210\n",
      "700/700 [==============================] - 1s 837us/sample - loss: 0.8517 - dense_194_loss: 0.4312 - dense_198_loss: 0.3180 - dense_194_accuracy: 0.8676 - dense_198_accuracy: 0.8939\n",
      "Epoch 76/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.8533 - dense_194_loss: 0.4374 - dense_198_loss: 0.3173 - dense_194_accuracy: 0.8651 - dense_198_accuracy: 0.8926\n",
      "Epoch 77/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.8087 - dense_194_loss: 0.4183 - dense_198_loss: 0.2996 - dense_194_accuracy: 0.8718 - dense_198_accuracy: 0.8960\n",
      "Epoch 78/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.7731 - dense_194_loss: 0.4094 - dense_198_loss: 0.2842 - dense_194_accuracy: 0.8744 - dense_198_accuracy: 0.9029\n",
      "Epoch 79/210\n",
      "700/700 [==============================] - 1s 837us/sample - loss: 0.7372 - dense_194_loss: 0.3940 - dense_198_loss: 0.2698 - dense_194_accuracy: 0.8775 - dense_198_accuracy: 0.9070\n",
      "Epoch 80/210\n",
      "700/700 [==============================] - 1s 837us/sample - loss: 0.7228 - dense_194_loss: 0.3919 - dense_198_loss: 0.2635 - dense_194_accuracy: 0.8793 - dense_198_accuracy: 0.9089\n",
      "Epoch 81/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.7471 - dense_194_loss: 0.3991 - dense_198_loss: 0.2737 - dense_194_accuracy: 0.8761 - dense_198_accuracy: 0.9050\n",
      "Epoch 82/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.7338 - dense_194_loss: 0.3921 - dense_198_loss: 0.2688 - dense_194_accuracy: 0.8791 - dense_198_accuracy: 0.9066\n",
      "Epoch 83/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.7094 - dense_194_loss: 0.3847 - dense_198_loss: 0.2586 - dense_194_accuracy: 0.8798 - dense_198_accuracy: 0.9091\n",
      "Epoch 84/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.7326 - dense_194_loss: 0.3964 - dense_198_loss: 0.2673 - dense_194_accuracy: 0.8759 - dense_198_accuracy: 0.9070\n",
      "Epoch 85/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.7133 - dense_194_loss: 0.3887 - dense_198_loss: 0.2595 - dense_194_accuracy: 0.8781 - dense_198_accuracy: 0.9084\n",
      "Epoch 86/210\n",
      "700/700 [==============================] - 1s 837us/sample - loss: 0.7607 - dense_194_loss: 0.4026 - dense_198_loss: 0.2799 - dense_194_accuracy: 0.8731 - dense_198_accuracy: 0.9035\n",
      "Epoch 87/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.7785 - dense_194_loss: 0.4206 - dense_198_loss: 0.2840 - dense_194_accuracy: 0.8676 - dense_198_accuracy: 0.9024\n",
      "Epoch 88/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.7197 - dense_194_loss: 0.3950 - dense_198_loss: 0.2610 - dense_194_accuracy: 0.8788 - dense_198_accuracy: 0.9088\n",
      "Epoch 89/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.6656 - dense_194_loss: 0.3754 - dense_198_loss: 0.2389 - dense_194_accuracy: 0.8832 - dense_198_accuracy: 0.9165\n",
      "Epoch 90/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.6367 - dense_194_loss: 0.3610 - dense_198_loss: 0.2279 - dense_194_accuracy: 0.8871 - dense_198_accuracy: 0.9186\n",
      "Epoch 91/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.6563 - dense_194_loss: 0.3726 - dense_198_loss: 0.2352 - dense_194_accuracy: 0.8845 - dense_198_accuracy: 0.9170\n",
      "Epoch 92/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.6653 - dense_194_loss: 0.3709 - dense_198_loss: 0.2401 - dense_194_accuracy: 0.8830 - dense_198_accuracy: 0.9155\n",
      "Epoch 93/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.6717 - dense_194_loss: 0.3748 - dense_198_loss: 0.2421 - dense_194_accuracy: 0.8828 - dense_198_accuracy: 0.9150\n",
      "Epoch 94/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.6474 - dense_194_loss: 0.3655 - dense_198_loss: 0.2323 - dense_194_accuracy: 0.8849 - dense_198_accuracy: 0.9186\n",
      "Epoch 95/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.6642 - dense_194_loss: 0.3637 - dense_198_loss: 0.2412 - dense_194_accuracy: 0.8848 - dense_198_accuracy: 0.9149\n",
      "Epoch 96/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5983 - dense_194_loss: 0.3462 - dense_198_loss: 0.2125 - dense_194_accuracy: 0.8916 - dense_198_accuracy: 0.9232\n",
      "Epoch 97/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.5902 - dense_194_loss: 0.3420 - dense_198_loss: 0.2097 - dense_194_accuracy: 0.8929 - dense_198_accuracy: 0.9253\n",
      "Epoch 98/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.5754 - dense_194_loss: 0.3381 - dense_198_loss: 0.2032 - dense_194_accuracy: 0.8944 - dense_198_accuracy: 0.9277\n",
      "Epoch 99/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5625 - dense_194_loss: 0.3357 - dense_198_loss: 0.1974 - dense_194_accuracy: 0.8939 - dense_198_accuracy: 0.9278\n",
      "Epoch 100/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5994 - dense_194_loss: 0.3470 - dense_198_loss: 0.2131 - dense_194_accuracy: 0.8889 - dense_198_accuracy: 0.9237\n",
      "Epoch 101/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.6040 - dense_194_loss: 0.3532 - dense_198_loss: 0.2136 - dense_194_accuracy: 0.8883 - dense_198_accuracy: 0.9240\n",
      "Epoch 102/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.5924 - dense_194_loss: 0.3508 - dense_198_loss: 0.2085 - dense_194_accuracy: 0.8878 - dense_198_accuracy: 0.9247\n",
      "Epoch 103/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5802 - dense_194_loss: 0.3392 - dense_198_loss: 0.2052 - dense_194_accuracy: 0.8916 - dense_198_accuracy: 0.9255\n",
      "Epoch 104/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.5348 - dense_194_loss: 0.3289 - dense_198_loss: 0.1851 - dense_194_accuracy: 0.8958 - dense_198_accuracy: 0.9328\n",
      "Epoch 105/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5241 - dense_194_loss: 0.3199 - dense_198_loss: 0.1820 - dense_194_accuracy: 0.8986 - dense_198_accuracy: 0.9326\n",
      "Epoch 106/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5000 - dense_194_loss: 0.3138 - dense_198_loss: 0.1717 - dense_194_accuracy: 0.9004 - dense_198_accuracy: 0.9368\n",
      "Epoch 107/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.4928 - dense_194_loss: 0.3077 - dense_198_loss: 0.1697 - dense_194_accuracy: 0.9010 - dense_198_accuracy: 0.9381\n",
      "Epoch 108/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4779 - dense_194_loss: 0.3028 - dense_198_loss: 0.1633 - dense_194_accuracy: 0.9028 - dense_198_accuracy: 0.9396\n",
      "Epoch 109/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5063 - dense_194_loss: 0.3109 - dense_198_loss: 0.1754 - dense_194_accuracy: 0.9001 - dense_198_accuracy: 0.9358\n",
      "Epoch 110/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5158 - dense_194_loss: 0.3148 - dense_198_loss: 0.1792 - dense_194_accuracy: 0.8992 - dense_198_accuracy: 0.9340\n",
      "Epoch 111/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.4885 - dense_194_loss: 0.3085 - dense_198_loss: 0.1671 - dense_194_accuracy: 0.9021 - dense_198_accuracy: 0.9381\n",
      "Epoch 112/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.4787 - dense_194_loss: 0.3028 - dense_198_loss: 0.1638 - dense_194_accuracy: 0.9032 - dense_198_accuracy: 0.9397\n",
      "Epoch 113/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.4782 - dense_194_loss: 0.3012 - dense_198_loss: 0.1639 - dense_194_accuracy: 0.9040 - dense_198_accuracy: 0.9391\n",
      "Epoch 114/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4643 - dense_194_loss: 0.2941 - dense_198_loss: 0.1586 - dense_194_accuracy: 0.9062 - dense_198_accuracy: 0.9411\n",
      "Epoch 115/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.4685 - dense_194_loss: 0.3009 - dense_198_loss: 0.1590 - dense_194_accuracy: 0.9041 - dense_198_accuracy: 0.9410\n",
      "Epoch 116/210\n",
      "700/700 [==============================] - 1s 850us/sample - loss: 0.4924 - dense_194_loss: 0.3087 - dense_198_loss: 0.1691 - dense_194_accuracy: 0.9007 - dense_198_accuracy: 0.9389\n",
      "Epoch 117/210\n",
      "700/700 [==============================] - 1s 869us/sample - loss: 0.5271 - dense_194_loss: 0.3262 - dense_198_loss: 0.1819 - dense_194_accuracy: 0.8951 - dense_198_accuracy: 0.9325\n",
      "Epoch 118/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5653 - dense_194_loss: 0.3332 - dense_198_loss: 0.1995 - dense_194_accuracy: 0.8936 - dense_198_accuracy: 0.9302\n",
      "Epoch 119/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5793 - dense_194_loss: 0.3465 - dense_198_loss: 0.2029 - dense_194_accuracy: 0.8887 - dense_198_accuracy: 0.9268\n",
      "Epoch 120/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.5444 - dense_194_loss: 0.3316 - dense_198_loss: 0.1891 - dense_194_accuracy: 0.8946 - dense_198_accuracy: 0.9318\n",
      "Epoch 121/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.5128 - dense_194_loss: 0.3192 - dense_198_loss: 0.1767 - dense_194_accuracy: 0.8961 - dense_198_accuracy: 0.9349\n",
      "Epoch 122/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4668 - dense_194_loss: 0.3002 - dense_198_loss: 0.1584 - dense_194_accuracy: 0.9032 - dense_198_accuracy: 0.9420\n",
      "Epoch 123/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.4418 - dense_194_loss: 0.2930 - dense_198_loss: 0.1477 - dense_194_accuracy: 0.9058 - dense_198_accuracy: 0.9445\n",
      "Epoch 124/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4272 - dense_194_loss: 0.2844 - dense_198_loss: 0.1424 - dense_194_accuracy: 0.9080 - dense_198_accuracy: 0.9465\n",
      "Epoch 125/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.4128 - dense_194_loss: 0.2784 - dense_198_loss: 0.1368 - dense_194_accuracy: 0.9097 - dense_198_accuracy: 0.9491\n",
      "Epoch 126/210\n",
      "700/700 [==============================] - 1s 913us/sample - loss: 0.3976 - dense_194_loss: 0.2735 - dense_198_loss: 0.1304 - dense_194_accuracy: 0.9102 - dense_198_accuracy: 0.9519s - loss: 0.4053 - dense_194_loss: 0.2773 - dense_198_loss: 0.1333 - dense_194_accuracy: 0.9109 - dense_198_accuracy: \n",
      "Epoch 127/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4006 - dense_194_loss: 0.2724 - dense_198_loss: 0.1322 - dense_194_accuracy: 0.9098 - dense_198_accuracy: 0.9504\n",
      "Epoch 128/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3927 - dense_194_loss: 0.2713 - dense_198_loss: 0.1285 - dense_194_accuracy: 0.9119 - dense_198_accuracy: 0.9516\n",
      "Epoch 129/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3781 - dense_194_loss: 0.2645 - dense_198_loss: 0.1229 - dense_194_accuracy: 0.9136 - dense_198_accuracy: 0.9538\n",
      "Epoch 130/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3756 - dense_194_loss: 0.2605 - dense_198_loss: 0.1227 - dense_194_accuracy: 0.9149 - dense_198_accuracy: 0.9538\n",
      "Epoch 131/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3763 - dense_194_loss: 0.2633 - dense_198_loss: 0.1224 - dense_194_accuracy: 0.9151 - dense_198_accuracy: 0.9538\n",
      "Epoch 132/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.3937 - dense_194_loss: 0.2644 - dense_198_loss: 0.1311 - dense_194_accuracy: 0.9129 - dense_198_accuracy: 0.9505\n",
      "Epoch 133/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3908 - dense_194_loss: 0.2655 - dense_198_loss: 0.1290 - dense_194_accuracy: 0.9134 - dense_198_accuracy: 0.9504\n",
      "Epoch 134/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3734 - dense_194_loss: 0.2618 - dense_198_loss: 0.1214 - dense_194_accuracy: 0.9151 - dense_198_accuracy: 0.9538\n",
      "Epoch 135/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.3590 - dense_194_loss: 0.2545 - dense_198_loss: 0.1160 - dense_194_accuracy: 0.9174 - dense_198_accuracy: 0.9556\n",
      "Epoch 136/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.3607 - dense_194_loss: 0.2552 - dense_198_loss: 0.1165 - dense_194_accuracy: 0.9161 - dense_198_accuracy: 0.9562\n",
      "Epoch 137/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3564 - dense_194_loss: 0.2517 - dense_198_loss: 0.1154 - dense_194_accuracy: 0.9178 - dense_198_accuracy: 0.9571\n",
      "Epoch 138/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4154 - dense_194_loss: 0.2744 - dense_198_loss: 0.1390 - dense_194_accuracy: 0.9093 - dense_198_accuracy: 0.9486\n",
      "Epoch 139/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4020 - dense_194_loss: 0.2762 - dense_198_loss: 0.1319 - dense_194_accuracy: 0.9091 - dense_198_accuracy: 0.9494\n",
      "Epoch 140/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.3690 - dense_194_loss: 0.2604 - dense_198_loss: 0.1195 - dense_194_accuracy: 0.9150 - dense_198_accuracy: 0.9544\n",
      "Epoch 141/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3765 - dense_194_loss: 0.2676 - dense_198_loss: 0.1213 - dense_194_accuracy: 0.9127 - dense_198_accuracy: 0.9530\n",
      "Epoch 142/210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4039 - dense_194_loss: 0.2714 - dense_198_loss: 0.1338 - dense_194_accuracy: 0.9105 - dense_198_accuracy: 0.9501\n",
      "Epoch 143/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3822 - dense_194_loss: 0.2638 - dense_198_loss: 0.1253 - dense_194_accuracy: 0.9138 - dense_198_accuracy: 0.9523\n",
      "Epoch 144/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.3611 - dense_194_loss: 0.2562 - dense_198_loss: 0.1165 - dense_194_accuracy: 0.9157 - dense_198_accuracy: 0.9564\n",
      "Epoch 145/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3674 - dense_194_loss: 0.2574 - dense_198_loss: 0.1194 - dense_194_accuracy: 0.9150 - dense_198_accuracy: 0.9550\n",
      "Epoch 146/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3416 - dense_194_loss: 0.2461 - dense_198_loss: 0.1093 - dense_194_accuracy: 0.9201 - dense_198_accuracy: 0.9584\n",
      "Epoch 147/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3302 - dense_194_loss: 0.2430 - dense_198_loss: 0.1044 - dense_194_accuracy: 0.9198 - dense_198_accuracy: 0.9603\n",
      "Epoch 148/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.3291 - dense_194_loss: 0.2411 - dense_198_loss: 0.1043 - dense_194_accuracy: 0.9196 - dense_198_accuracy: 0.9605\n",
      "Epoch 149/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3195 - dense_194_loss: 0.2397 - dense_198_loss: 0.0998 - dense_194_accuracy: 0.9214 - dense_198_accuracy: 0.9617\n",
      "Epoch 150/210\n",
      "700/700 [==============================] - 1s 837us/sample - loss: 0.3176 - dense_194_loss: 0.2398 - dense_198_loss: 0.0988 - dense_194_accuracy: 0.9222 - dense_198_accuracy: 0.9630\n",
      "Epoch 151/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.3231 - dense_194_loss: 0.2424 - dense_198_loss: 0.1010 - dense_194_accuracy: 0.9196 - dense_198_accuracy: 0.9618\n",
      "Epoch 152/210\n",
      "700/700 [==============================] - 1s 869us/sample - loss: 0.3081 - dense_194_loss: 0.2334 - dense_198_loss: 0.0956 - dense_194_accuracy: 0.9234 - dense_198_accuracy: 0.9634\n",
      "Epoch 153/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3199 - dense_194_loss: 0.2374 - dense_198_loss: 0.1007 - dense_194_accuracy: 0.9217 - dense_198_accuracy: 0.9607\n",
      "Epoch 154/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3159 - dense_194_loss: 0.2369 - dense_198_loss: 0.0986 - dense_194_accuracy: 0.9221 - dense_198_accuracy: 0.9626\n",
      "Epoch 155/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.3111 - dense_194_loss: 0.2334 - dense_198_loss: 0.0972 - dense_194_accuracy: 0.9221 - dense_198_accuracy: 0.9626\n",
      "Epoch 156/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2987 - dense_194_loss: 0.2326 - dense_198_loss: 0.0912 - dense_194_accuracy: 0.9229 - dense_198_accuracy: 0.9652\n",
      "Epoch 157/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2906 - dense_194_loss: 0.2265 - dense_198_loss: 0.0887 - dense_194_accuracy: 0.9254 - dense_198_accuracy: 0.9670\n",
      "Epoch 158/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2976 - dense_194_loss: 0.2292 - dense_198_loss: 0.0914 - dense_194_accuracy: 0.9238 - dense_198_accuracy: 0.9647\n",
      "Epoch 159/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.2976 - dense_194_loss: 0.2280 - dense_198_loss: 0.0917 - dense_194_accuracy: 0.9240 - dense_198_accuracy: 0.9645\n",
      "Epoch 160/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.3121 - dense_194_loss: 0.2343 - dense_198_loss: 0.0975 - dense_194_accuracy: 0.9229 - dense_198_accuracy: 0.9630\n",
      "Epoch 161/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3244 - dense_194_loss: 0.2410 - dense_198_loss: 0.1019 - dense_194_accuracy: 0.9212 - dense_198_accuracy: 0.9613\n",
      "Epoch 162/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3438 - dense_194_loss: 0.2500 - dense_198_loss: 0.1095 - dense_194_accuracy: 0.9180 - dense_198_accuracy: 0.9597\n",
      "Epoch 163/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3996 - dense_194_loss: 0.2657 - dense_198_loss: 0.1335 - dense_194_accuracy: 0.9118 - dense_198_accuracy: 0.9514\n",
      "Epoch 164/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.4543 - dense_194_loss: 0.2865 - dense_198_loss: 0.1554 - dense_194_accuracy: 0.9062 - dense_198_accuracy: 0.9445\n",
      "Epoch 165/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4389 - dense_194_loss: 0.2809 - dense_198_loss: 0.1494 - dense_194_accuracy: 0.9071 - dense_198_accuracy: 0.9453\n",
      "Epoch 166/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4618 - dense_194_loss: 0.2894 - dense_198_loss: 0.1585 - dense_194_accuracy: 0.9042 - dense_198_accuracy: 0.9429\n",
      "Epoch 167/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.4090 - dense_194_loss: 0.2768 - dense_198_loss: 0.1353 - dense_194_accuracy: 0.9095 - dense_198_accuracy: 0.9498\n",
      "Epoch 168/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4392 - dense_194_loss: 0.2929 - dense_198_loss: 0.1464 - dense_194_accuracy: 0.9050 - dense_198_accuracy: 0.9471\n",
      "Epoch 169/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.4058 - dense_194_loss: 0.2745 - dense_198_loss: 0.1343 - dense_194_accuracy: 0.9100 - dense_198_accuracy: 0.9503\n",
      "Epoch 170/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3682 - dense_194_loss: 0.2594 - dense_198_loss: 0.1193 - dense_194_accuracy: 0.9148 - dense_198_accuracy: 0.9548\n",
      "Epoch 171/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.3444 - dense_194_loss: 0.2493 - dense_198_loss: 0.1098 - dense_194_accuracy: 0.9187 - dense_198_accuracy: 0.9580\n",
      "Epoch 172/210\n",
      "700/700 [==============================] - 1s 869us/sample - loss: 0.3046 - dense_194_loss: 0.2320 - dense_198_loss: 0.0942 - dense_194_accuracy: 0.9229 - dense_198_accuracy: 0.9645\n",
      "Epoch 173/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2895 - dense_194_loss: 0.2271 - dense_198_loss: 0.0880 - dense_194_accuracy: 0.9254 - dense_198_accuracy: 0.9670\n",
      "Epoch 174/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2816 - dense_194_loss: 0.2229 - dense_198_loss: 0.0851 - dense_194_accuracy: 0.9253 - dense_198_accuracy: 0.9679\n",
      "Epoch 175/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2636 - dense_194_loss: 0.2171 - dense_198_loss: 0.0775 - dense_194_accuracy: 0.9285 - dense_198_accuracy: 0.9711\n",
      "Epoch 176/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2675 - dense_194_loss: 0.2140 - dense_198_loss: 0.0802 - dense_194_accuracy: 0.9288 - dense_198_accuracy: 0.9693\n",
      "Epoch 177/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2619 - dense_194_loss: 0.2120 - dense_198_loss: 0.0780 - dense_194_accuracy: 0.9300 - dense_198_accuracy: 0.9711\n",
      "Epoch 178/210\n",
      "700/700 [==============================] - 1s 860us/sample - loss: 0.2529 - dense_194_loss: 0.2088 - dense_198_loss: 0.0743 - dense_194_accuracy: 0.9308 - dense_198_accuracy: 0.9719\n",
      "Epoch 179/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2640 - dense_194_loss: 0.2119 - dense_198_loss: 0.0790 - dense_194_accuracy: 0.9299 - dense_198_accuracy: 0.9697\n",
      "Epoch 180/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.3193 - dense_194_loss: 0.2348 - dense_198_loss: 0.1011 - dense_194_accuracy: 0.9229 - dense_198_accuracy: 0.9638\n",
      "Epoch 181/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3515 - dense_194_loss: 0.2472 - dense_198_loss: 0.1140 - dense_194_accuracy: 0.9178 - dense_198_accuracy: 0.9596\n",
      "Epoch 182/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.3430 - dense_194_loss: 0.2424 - dense_198_loss: 0.1109 - dense_194_accuracy: 0.9198 - dense_198_accuracy: 0.9586\n",
      "Epoch 183/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.3069 - dense_194_loss: 0.2317 - dense_198_loss: 0.0955 - dense_194_accuracy: 0.9231 - dense_198_accuracy: 0.9641\n",
      "Epoch 184/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2858 - dense_194_loss: 0.2211 - dense_198_loss: 0.0878 - dense_194_accuracy: 0.9277 - dense_198_accuracy: 0.9666\n",
      "Epoch 185/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2877 - dense_194_loss: 0.2286 - dense_198_loss: 0.0866 - dense_194_accuracy: 0.9246 - dense_198_accuracy: 0.9677\n",
      "Epoch 186/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2819 - dense_194_loss: 0.2183 - dense_198_loss: 0.0863 - dense_194_accuracy: 0.9280 - dense_198_accuracy: 0.9669\n",
      "Epoch 187/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2804 - dense_194_loss: 0.2183 - dense_198_loss: 0.0858 - dense_194_accuracy: 0.9277 - dense_198_accuracy: 0.9683\n",
      "Epoch 188/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2956 - dense_194_loss: 0.2266 - dense_198_loss: 0.0911 - dense_194_accuracy: 0.9244 - dense_198_accuracy: 0.9662\n",
      "Epoch 189/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2865 - dense_194_loss: 0.2211 - dense_198_loss: 0.0881 - dense_194_accuracy: 0.9263 - dense_198_accuracy: 0.9661\n",
      "Epoch 190/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2671 - dense_194_loss: 0.2134 - dense_198_loss: 0.0801 - dense_194_accuracy: 0.9290 - dense_198_accuracy: 0.9696\n",
      "Epoch 191/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2605 - dense_194_loss: 0.2129 - dense_198_loss: 0.0772 - dense_194_accuracy: 0.9302 - dense_198_accuracy: 0.9710\n",
      "Epoch 192/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2551 - dense_194_loss: 0.2091 - dense_198_loss: 0.0752 - dense_194_accuracy: 0.9314 - dense_198_accuracy: 0.9709\n",
      "Epoch 193/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2476 - dense_194_loss: 0.2068 - dense_198_loss: 0.0721 - dense_194_accuracy: 0.9311 - dense_198_accuracy: 0.9725\n",
      "Epoch 194/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2424 - dense_194_loss: 0.2018 - dense_198_loss: 0.0708 - dense_194_accuracy: 0.9328 - dense_198_accuracy: 0.9728\n",
      "Epoch 195/210\n",
      "700/700 [==============================] - 1s 869us/sample - loss: 0.2318 - dense_194_loss: 0.1947 - dense_198_loss: 0.0672 - dense_194_accuracy: 0.9358 - dense_198_accuracy: 0.9743\n",
      "Epoch 196/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2302 - dense_194_loss: 0.1924 - dense_198_loss: 0.0671 - dense_194_accuracy: 0.9345 - dense_198_accuracy: 0.9747\n",
      "Epoch 197/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2332 - dense_194_loss: 0.1941 - dense_198_loss: 0.0681 - dense_194_accuracy: 0.9351 - dense_198_accuracy: 0.9739\n",
      "Epoch 198/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2355 - dense_194_loss: 0.1971 - dense_198_loss: 0.0685 - dense_194_accuracy: 0.9344 - dense_198_accuracy: 0.9741\n",
      "Epoch 199/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2236 - dense_194_loss: 0.1932 - dense_198_loss: 0.0634 - dense_194_accuracy: 0.9356 - dense_198_accuracy: 0.9756\n",
      "Epoch 200/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2166 - dense_194_loss: 0.1882 - dense_198_loss: 0.0613 - dense_194_accuracy: 0.9370 - dense_198_accuracy: 0.9773\n",
      "Epoch 201/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2230 - dense_194_loss: 0.1888 - dense_198_loss: 0.0643 - dense_194_accuracy: 0.9370 - dense_198_accuracy: 0.9760\n",
      "Epoch 202/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2353 - dense_194_loss: 0.1943 - dense_198_loss: 0.0690 - dense_194_accuracy: 0.9350 - dense_198_accuracy: 0.9734\n",
      "Epoch 203/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2264 - dense_194_loss: 0.1888 - dense_198_loss: 0.0660 - dense_194_accuracy: 0.9374 - dense_198_accuracy: 0.9750\n",
      "Epoch 204/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2194 - dense_194_loss: 0.1912 - dense_198_loss: 0.0619 - dense_194_accuracy: 0.9360 - dense_198_accuracy: 0.9762\n",
      "Epoch 205/210\n",
      "700/700 [==============================] - 1s 859us/sample - loss: 0.2201 - dense_194_loss: 0.1887 - dense_198_loss: 0.0629 - dense_194_accuracy: 0.9370 - dense_198_accuracy: 0.9767\n",
      "Epoch 206/210\n",
      "700/700 [==============================] - 1s 869us/sample - loss: 0.2100 - dense_194_loss: 0.1847 - dense_198_loss: 0.0588 - dense_194_accuracy: 0.9385 - dense_198_accuracy: 0.9783\n",
      "Epoch 207/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2078 - dense_194_loss: 0.1823 - dense_198_loss: 0.0583 - dense_194_accuracy: 0.9384 - dense_198_accuracy: 0.9780\n",
      "Epoch 208/210\n",
      "700/700 [==============================] - 1s 882us/sample - loss: 0.2108 - dense_194_loss: 0.1869 - dense_198_loss: 0.0587 - dense_194_accuracy: 0.9375 - dense_198_accuracy: 0.9774\n",
      "Epoch 209/210\n",
      "700/700 [==============================] - 1s 904us/sample - loss: 0.2348 - dense_194_loss: 0.1912 - dense_198_loss: 0.0696 - dense_194_accuracy: 0.9360 - dense_198_accuracy: 0.9733\n",
      "Epoch 210/210\n",
      "700/700 [==============================] - 1s 904us/sample - loss: 0.2312 - dense_194_loss: 0.1934 - dense_198_loss: 0.0673 - dense_194_accuracy: 0.9354 - dense_198_accuracy: 0.9745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f19a0b7848>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "ip = keras.layers.Input(shape = (75, 2048), batch_size = None)\n",
    "ip_drop = keras.layers.Dropout(0)(ip)\n",
    "ip_red1 = keras.layers.Dense(256*2, activation=\"relu\")(ip_drop)\n",
    "ip_red1_drop = keras.layers.Dropout(0.3)(ip_red1)\n",
    "ip_red2 = keras.layers.Dense(128*2, activation=\"relu\")(ip_red1_drop)\n",
    "ip_red2_drop = keras.layers.Dropout(0.2)(ip_red2)\n",
    "bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(64*2, return_sequences=True))\n",
    "a1 = bi_lstm(ip_red2_drop)\n",
    "\n",
    "t_model1 = keras.models.Model(inputs=ip, outputs=a1)\n",
    "\n",
    "attn_res = keras.layers.Attention()([a1, a1])\n",
    "\n",
    "t_model2 = keras.models.Model(inputs=ip, outputs=attn_res)\n",
    "\n",
    "res = tf.concat([a1, attn_res], axis = -1)\n",
    "res = keras.layers.Dropout(0.3)(res)\n",
    "\n",
    "# x = keras.layers.Dense(512, activation=\"relu\")(res)\n",
    "# x=keras.layers.Dropout(0.2)(x)\n",
    "a2 = keras.layers.Dense(256, activation=\"relu\")(res)\n",
    "a2=keras.layers.Dropout(0.3)(a2)\n",
    "a3 = keras.layers.Dense(128, activation=\"relu\")(a2)\n",
    "a4 = keras.layers.Dense(len(vocab), activation=\"softmax\")(a3)\n",
    "a4_t = keras.layers.Dense(128, activation=\"relu\")(a4)\n",
    "bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))\n",
    "a5 = bi_lstm(a4_t)\n",
    "\n",
    "t_model3 = keras.models.Model(inputs=ip, outputs=a5)\n",
    "\n",
    "attn_res2 = keras.layers.Attention()([a5, a5])\n",
    "\n",
    "t_model4 = keras.models.Model(inputs=ip, outputs=attn_res2)\n",
    "\n",
    "res2 = tf.concat([a5, attn_res2], axis = -1)\n",
    "x = keras.layers.Dense(256, activation=\"relu\")(res2)\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "# op = keras.layers.Dense(len(vocab), activation=\"softmax\")(tf.concat([x, a3], axis = -1))\n",
    "op = keras.layers.Dense(len(vocab), activation=\"softmax\")(x)\n",
    "\n",
    "model4=keras.models.Model(inputs=ip, outputs=[a4, op])\n",
    "model4.summary()\n",
    "model4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              loss_weights=[0.5, 2])\n",
    "\n",
    "model4.fit(x_train,[y_train, y_train], batch_size=64, epochs=210)\n",
    "# print(t_model1.summary(),end=\"\\n--------------\\n\")\n",
    "# print(t_model2.summary(),end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "ip = keras.layers.Input(shape = (75, 2048), batch_size = None)\n",
    "ip_drop = keras.layers.Dropout(0)(ip)\n",
    "ip_red1 = keras.layers.Dense(256*2, activation=\"relu\")(ip_drop)\n",
    "ip_red1_drop = keras.layers.Dropout(0.3)(ip_red1)\n",
    "ip_red2 = keras.layers.Dense(128*2, activation=\"relu\")(ip_red1_drop)\n",
    "ip_red2_drop = keras.layers.Dropout(0.2)(ip_red2)\n",
    "bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(64*2, return_sequences=True))\n",
    "a1 = bi_lstm(ip_red2_drop)\n",
    "\n",
    "t_model1 = keras.models.Model(inputs=ip, outputs=a1)\n",
    "\n",
    "attn_res = keras.layers.Attention()([a1, a1])\n",
    "\n",
    "t_model2 = keras.models.Model(inputs=ip, outputs=attn_res)\n",
    "\n",
    "res = tf.concat([a1, attn_res], axis = -1)\n",
    "res = keras.layers.Dropout(0.3)(res)\n",
    "\n",
    "# x = keras.layers.Dense(512, activation=\"relu\")(res)\n",
    "# x=keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))\n",
    "a5 = bi_lstm(res)\n",
    "\n",
    "t_model3 = keras.models.Model(inputs=ip, outputs=a5)\n",
    "\n",
    "attn_res2 = keras.layers.Attention()([a5, a5])\n",
    "\n",
    "t_model4 = keras.models.Model(inputs=ip, outputs=attn_res2)\n",
    "\n",
    "res2 = tf.concat([a5, attn_res2], axis = -1)\n",
    "\n",
    "bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))\n",
    "a5 = bi_lstm(res2)\n",
    "\n",
    "t_model5 = keras.models.Model(inputs=ip, outputs=a5)\n",
    "\n",
    "attn_res2 = keras.layers.Attention()([a5, a5])\n",
    "\n",
    "t_model6 = keras.models.Model(inputs=ip, outputs=attn_res2)\n",
    "\n",
    "res3 = tf.concat([a5, attn_res2], axis = -1)\n",
    "\n",
    "bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))\n",
    "a5 = bi_lstm(res3)\n",
    "\n",
    "t_model7 = keras.models.Model(inputs=ip, outputs=a5)\n",
    "\n",
    "attn_res2 = keras.layers.Attention()([a5, a5])\n",
    "\n",
    "t_model8 = keras.models.Model(inputs=ip, outputs=attn_res2)\n",
    "\n",
    "res4 = tf.concat([a5, attn_res2], axis = -1)\n",
    "\n",
    "x = keras.layers.Dense(256, activation=\"relu\")(res4)\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "# op = keras.layers.Dense(len(vocab), activation=\"softmax\")(tf.concat([x, a3], axis = -1))\n",
    "op = keras.layers.Dense(len(vocab), activation=\"softmax\")(x)\n",
    "\n",
    "model4=keras.models.Model(inputs=ip, outputs=op)\n",
    "model4.summary()\n",
    "model4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model4.fit(x_train, y_train, batch_size=64, epochs=210)\n",
    "# print(t_model1.summary(),end=\"\\n--------------\\n\")\n",
    "# print(t_model2.summary(),end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>lang model 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for key in aligns:\n",
    "    sent = \" \".join(list(aligns[key].loc[:, \"utter\"])).strip()\n",
    "    sent = sent.replace(\" sp\", \"\")\n",
    "    sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents[132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lang_x = np.zeros((len(sents), 7, len(vocab)))\n",
    "lang_y = np.zeros((len(sents), 7, len(vocab)))\n",
    "\n",
    "for i, sent in enumerate(sents):\n",
    "    print(i)\n",
    "    for j, word in enumerate(sent.split()):\n",
    "        if j < 7:\n",
    "            lang_x[i, j, vocab.index(word)] = 1\n",
    "            \n",
    "    for j, word in enumerate(sent.split()):\n",
    "#         print(j)\n",
    "        if j > 0:\n",
    "            lang_y[i, j - 1, vocab.index(word)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "ip = keras.layers.Input((None, len(vocab)))\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(ip)\n",
    "x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "op = keras.layers.Dense(len(vocab), activation=\"softmax\")(x)\n",
    "\n",
    "lang_model = keras.models.Model(inputs=ip, outputs=op)\n",
    "lang_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lang_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lang_model.fit(lang_x, lang_y, batch_size=64, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sent):\n",
    "    global vocab\n",
    "    res = np.zeros((1, len(sent.split()), 53))\n",
    "    for i, word in enumerate(sent.split()):\n",
    "        res[0, i, vocab.index(word)] = 1\n",
    "    return res\n",
    "\n",
    "def decode(pred):\n",
    "    pred = pred[0]\n",
    "    ret = \"\"\n",
    "    for t in pred:\n",
    "         ret += \" \" + vocab[np.argmax(t)]\n",
    "    return ret\n",
    "\n",
    "sample = sents[10]\n",
    "\n",
    "hist = \"\"\n",
    "for word in sample.split():\n",
    "    hist += \" \" + word\n",
    "    hist = hist.strip()\n",
    "    temp = lang_model.predict(encode(word))\n",
    "    pred = decode(temp)\n",
    "    temp = temp[0, -1]    \n",
    "    print(\"hist:\", hist)\n",
    "    s = np.argsort(temp)\n",
    "    s=s[-5:]\n",
    "    for si in s:\n",
    "        print(vocab[si], temp[si],  end=\",\")\n",
    "        \n",
    "    print(\"\\npred:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lang model 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for key in aligns:\n",
    "    sent = \" \".join(list(aligns[key].loc[:, \"utter\"])).strip()\n",
    "    sent = sent.replace(\" sp\", \"\")\n",
    "    sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "lang_x = np.zeros((len(sents) * 7, len(vocab)))\n",
    "lang_y = np.zeros((len(sents) * 7, len(vocab)))\n",
    "\n",
    "for i, sent in enumerate(sents):\n",
    "    print(i)\n",
    "    for j, word in enumerate(sent.split()):\n",
    "        if j == 7:\n",
    "            break\n",
    "        lang_x[i*7 + j, vocab.index(word)] = 1\n",
    "            \n",
    "    for j, word in enumerate(sent.split()):\n",
    "        if j == 0:\n",
    "            continue\n",
    "        lang_y[i*7 + j - 1, vocab.index(word)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(69)\n",
    "np.random.shuffle(lang_x)\n",
    "np.random.seed(69)\n",
    "np.random.shuffle(lang_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 53)]              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               6912      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 53)                6837      \n",
      "=================================================================\n",
      "Total params: 79,669\n",
      "Trainable params: 79,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7000 samples\n",
      "Epoch 1/150\n",
      "7000/7000 [==============================] - 1s 93us/sample - loss: 2.4209 - accuracy: 0.2581\n",
      "Epoch 2/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.5555 - accuracy: 0.3333\n",
      "Epoch 3/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.5218 - accuracy: 0.3303\n",
      "Epoch 4/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.5140 - accuracy: 0.3327\n",
      "Epoch 5/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.5102 - accuracy: 0.3397\n",
      "Epoch 6/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.5106 - accuracy: 0.3261\n",
      "Epoch 7/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.5056 - accuracy: 0.3420\n",
      "Epoch 8/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.5058 - accuracy: 0.3306\n",
      "Epoch 9/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.5027 - accuracy: 0.3346\n",
      "Epoch 10/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.5035 - accuracy: 0.3313\n",
      "Epoch 11/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.5005 - accuracy: 0.3373\n",
      "Epoch 12/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4993 - accuracy: 0.3353\n",
      "Epoch 13/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4990 - accuracy: 0.3444\n",
      "Epoch 14/150\n",
      "7000/7000 [==============================] - 0s 37us/sample - loss: 1.4974 - accuracy: 0.3437\n",
      "Epoch 15/150\n",
      "7000/7000 [==============================] - 0s 37us/sample - loss: 1.4979 - accuracy: 0.3460\n",
      "Epoch 16/150\n",
      "7000/7000 [==============================] - 0s 37us/sample - loss: 1.4958 - accuracy: 0.3297\n",
      "Epoch 17/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4972 - accuracy: 0.3373\n",
      "Epoch 18/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4989 - accuracy: 0.3284\n",
      "Epoch 19/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4945 - accuracy: 0.3383\n",
      "Epoch 20/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.4951 - accuracy: 0.3291\n",
      "Epoch 21/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4959 - accuracy: 0.3363\n",
      "Epoch 22/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4952 - accuracy: 0.3350\n",
      "Epoch 23/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4959 - accuracy: 0.3290\n",
      "Epoch 24/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4922 - accuracy: 0.3349\n",
      "Epoch 25/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4928 - accuracy: 0.3334\n",
      "Epoch 26/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4918 - accuracy: 0.3429\n",
      "Epoch 27/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.4928 - accuracy: 0.3361\n",
      "Epoch 28/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4927 - accuracy: 0.3390\n",
      "Epoch 29/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4928 - accuracy: 0.3310\n",
      "Epoch 30/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4913 - accuracy: 0.3369\n",
      "Epoch 31/150\n",
      "7000/7000 [==============================] - 0s 36us/sample - loss: 1.4914 - accuracy: 0.3356\n",
      "Epoch 32/150\n",
      "7000/7000 [==============================] - 0s 39us/sample - loss: 1.4922 - accuracy: 0.3337\n",
      "Epoch 33/150\n",
      "7000/7000 [==============================] - 0s 39us/sample - loss: 1.4902 - accuracy: 0.3320\n",
      "Epoch 34/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4908 - accuracy: 0.3294\n",
      "Epoch 35/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4915 - accuracy: 0.3343\n",
      "Epoch 36/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4903 - accuracy: 0.3303\n",
      "Epoch 37/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.4899 - accuracy: 0.3349\n",
      "Epoch 38/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4899 - accuracy: 0.3341\n",
      "Epoch 39/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4894 - accuracy: 0.3290\n",
      "Epoch 40/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4890 - accuracy: 0.3366\n",
      "Epoch 41/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4901 - accuracy: 0.3393\n",
      "Epoch 42/150\n",
      "7000/7000 [==============================] - 0s 37us/sample - loss: 1.4889 - accuracy: 0.3403\n",
      "Epoch 43/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4895 - accuracy: 0.3307\n",
      "Epoch 44/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4891 - accuracy: 0.3330\n",
      "Epoch 45/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4889 - accuracy: 0.3339\n",
      "Epoch 46/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4890 - accuracy: 0.3293\n",
      "Epoch 47/150\n",
      "7000/7000 [==============================] - 0s 36us/sample - loss: 1.4885 - accuracy: 0.3314\n",
      "Epoch 48/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4887 - accuracy: 0.3363\n",
      "Epoch 49/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4885 - accuracy: 0.3390\n",
      "Epoch 50/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4880 - accuracy: 0.3294\n",
      "Epoch 51/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4870 - accuracy: 0.3371\n",
      "Epoch 52/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4882 - accuracy: 0.3349\n",
      "Epoch 53/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4880 - accuracy: 0.3313\n",
      "Epoch 54/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4871 - accuracy: 0.3371\n",
      "Epoch 55/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4875 - accuracy: 0.3363\n",
      "Epoch 56/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4879 - accuracy: 0.3303\n",
      "Epoch 57/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4882 - accuracy: 0.3237\n",
      "Epoch 58/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4866 - accuracy: 0.3326\n",
      "Epoch 59/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4876 - accuracy: 0.3309\n",
      "Epoch 60/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4866 - accuracy: 0.3343\n",
      "Epoch 61/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4871 - accuracy: 0.3364\n",
      "Epoch 62/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4866 - accuracy: 0.3341\n",
      "Epoch 63/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4873 - accuracy: 0.3291\n",
      "Epoch 64/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4864 - accuracy: 0.3397\n",
      "Epoch 65/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4866 - accuracy: 0.3379\n",
      "Epoch 66/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4870 - accuracy: 0.3323\n",
      "Epoch 67/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4867 - accuracy: 0.3359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4864 - accuracy: 0.3364\n",
      "Epoch 69/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4868 - accuracy: 0.3283\n",
      "Epoch 70/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.4858 - accuracy: 0.3387\n",
      "Epoch 71/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4873 - accuracy: 0.3346\n",
      "Epoch 72/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4871 - accuracy: 0.3279\n",
      "Epoch 73/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4857 - accuracy: 0.3386\n",
      "Epoch 74/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4858 - accuracy: 0.3377\n",
      "Epoch 75/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4857 - accuracy: 0.3320\n",
      "Epoch 76/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4861 - accuracy: 0.3341\n",
      "Epoch 77/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4863 - accuracy: 0.3274\n",
      "Epoch 78/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4861 - accuracy: 0.3273\n",
      "Epoch 79/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4861 - accuracy: 0.3310\n",
      "Epoch 80/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4855 - accuracy: 0.3376\n",
      "Epoch 81/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4855 - accuracy: 0.3337\n",
      "Epoch 82/150\n",
      "7000/7000 [==============================] - 0s 37us/sample - loss: 1.4858 - accuracy: 0.3357\n",
      "Epoch 83/150\n",
      "7000/7000 [==============================] - 0s 40us/sample - loss: 1.4859 - accuracy: 0.3293\n",
      "Epoch 84/150\n",
      "7000/7000 [==============================] - 0s 40us/sample - loss: 1.4854 - accuracy: 0.3341\n",
      "Epoch 85/150\n",
      "7000/7000 [==============================] - 0s 36us/sample - loss: 1.4854 - accuracy: 0.3299\n",
      "Epoch 86/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4855 - accuracy: 0.3343\n",
      "Epoch 87/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4849 - accuracy: 0.3349\n",
      "Epoch 88/150\n",
      "7000/7000 [==============================] - 0s 37us/sample - loss: 1.4856 - accuracy: 0.3269\n",
      "Epoch 89/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4844 - accuracy: 0.3246\n",
      "Epoch 90/150\n",
      "7000/7000 [==============================] - 0s 40us/sample - loss: 1.4845 - accuracy: 0.3319\n",
      "Epoch 91/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4855 - accuracy: 0.3359\n",
      "Epoch 92/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4853 - accuracy: 0.3261\n",
      "Epoch 93/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4851 - accuracy: 0.3300\n",
      "Epoch 94/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4851 - accuracy: 0.3281\n",
      "Epoch 95/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4846 - accuracy: 0.3393\n",
      "Epoch 96/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4851 - accuracy: 0.3346\n",
      "Epoch 97/150\n",
      "7000/7000 [==============================] - 0s 39us/sample - loss: 1.4854 - accuracy: 0.3331\n",
      "Epoch 98/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4851 - accuracy: 0.3291\n",
      "Epoch 99/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4845 - accuracy: 0.3300\n",
      "Epoch 100/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4844 - accuracy: 0.3300\n",
      "Epoch 101/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4842 - accuracy: 0.3359\n",
      "Epoch 102/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4850 - accuracy: 0.3330\n",
      "Epoch 103/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4843 - accuracy: 0.3349\n",
      "Epoch 104/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4850 - accuracy: 0.3303\n",
      "Epoch 105/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4843 - accuracy: 0.3321\n",
      "Epoch 106/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4852 - accuracy: 0.3296\n",
      "Epoch 107/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4844 - accuracy: 0.3291\n",
      "Epoch 108/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4842 - accuracy: 0.3350\n",
      "Epoch 109/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4844 - accuracy: 0.3337\n",
      "Epoch 110/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4838 - accuracy: 0.3369\n",
      "Epoch 111/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4843 - accuracy: 0.3286\n",
      "Epoch 112/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4838 - accuracy: 0.3356\n",
      "Epoch 113/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4853 - accuracy: 0.3356\n",
      "Epoch 114/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4851 - accuracy: 0.3346\n",
      "Epoch 115/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4845 - accuracy: 0.3337\n",
      "Epoch 116/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4841 - accuracy: 0.3364\n",
      "Epoch 117/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4846 - accuracy: 0.3294\n",
      "Epoch 118/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4841 - accuracy: 0.3326\n",
      "Epoch 119/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4843 - accuracy: 0.3369\n",
      "Epoch 120/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4838 - accuracy: 0.3294\n",
      "Epoch 121/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4842 - accuracy: 0.3334\n",
      "Epoch 122/150\n",
      "7000/7000 [==============================] - 0s 39us/sample - loss: 1.4841 - accuracy: 0.3311\n",
      "Epoch 123/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4840 - accuracy: 0.3271\n",
      "Epoch 124/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4836 - accuracy: 0.3334\n",
      "Epoch 125/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.4839 - accuracy: 0.3307\n",
      "Epoch 126/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4834 - accuracy: 0.3317\n",
      "Epoch 127/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4839 - accuracy: 0.3243\n",
      "Epoch 128/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4841 - accuracy: 0.3351\n",
      "Epoch 129/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4837 - accuracy: 0.3323\n",
      "Epoch 130/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4838 - accuracy: 0.3321\n",
      "Epoch 131/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4832 - accuracy: 0.3391\n",
      "Epoch 132/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4840 - accuracy: 0.3261\n",
      "Epoch 133/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4840 - accuracy: 0.3323\n",
      "Epoch 134/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4840 - accuracy: 0.3340\n",
      "Epoch 135/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.4837 - accuracy: 0.3359\n",
      "Epoch 136/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4839 - accuracy: 0.3363\n",
      "Epoch 137/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4838 - accuracy: 0.3321\n",
      "Epoch 138/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4835 - accuracy: 0.3323\n",
      "Epoch 139/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4837 - accuracy: 0.3299\n",
      "Epoch 140/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4836 - accuracy: 0.3316\n",
      "Epoch 141/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.4837 - accuracy: 0.3287\n",
      "Epoch 142/150\n",
      "7000/7000 [==============================] - 0s 34us/sample - loss: 1.4842 - accuracy: 0.3293\n",
      "Epoch 143/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4835 - accuracy: 0.3319\n",
      "Epoch 144/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4836 - accuracy: 0.3267\n",
      "Epoch 145/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4838 - accuracy: 0.3300\n",
      "Epoch 146/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4837 - accuracy: 0.3290\n",
      "Epoch 147/150\n",
      "7000/7000 [==============================] - 0s 31us/sample - loss: 1.4837 - accuracy: 0.3366\n",
      "Epoch 148/150\n",
      "7000/7000 [==============================] - 0s 33us/sample - loss: 1.4835 - accuracy: 0.3303\n",
      "Epoch 149/150\n",
      "7000/7000 [==============================] - 0s 36us/sample - loss: 1.4836 - accuracy: 0.3280\n",
      "Epoch 150/150\n",
      "7000/7000 [==============================] - 0s 37us/sample - loss: 1.4832 - accuracy: 0.3319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ef2f2f14c8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "ip = keras.layers.Input(len(vocab))\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(ip)\n",
    "x = keras.layers.Dense(256, activation=\"tanh\")(x)\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "op = keras.layers.Dense(len(vocab), activation=\"softmax\")(x)\n",
    "\n",
    "lang_model = keras.models.Model(inputs=ip, outputs=op)\n",
    "lang_model.summary()\n",
    "\n",
    "lang_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lang_model.fit(lang_x, lang_y, batch_size=64, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sent):\n",
    "    global vocab\n",
    "    res = np.zeros((1, 53))\n",
    "    for i, word in enumerate(sent.split()):\n",
    "        res[0, vocab.index(word)] = 1\n",
    "    return res\n",
    "\n",
    "def decode(pred):\n",
    "    pred = pred[0]\n",
    "    return vocab[np.argmax(pred)]\n",
    "\n",
    "sample = sents[10]\n",
    "\n",
    "for word in sample.split():\n",
    "    temp = lang_model.predict(encode(word))\n",
    "    pred = decode(temp)\n",
    "    temp = temp[0]    \n",
    "    print(\"word:\", word)\n",
    "    s = np.argsort(temp)\n",
    "    s=s[-5:]\n",
    "    for si in s:\n",
    "        print(vocab[si], temp[si],  end=\",\")   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>final</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "ip = keras.layers.Input(shape = (75, 2048), batch_size = None)\n",
    "ip_drop = keras.layers.Dropout(0)(ip)\n",
    "ip_red1 = keras.layers.Dense(512, activation=\"relu\")(ip_drop)\n",
    "ip_red1_drop = keras.layers.Dropout(0.3)(ip_red1)\n",
    "ip_red2 = keras.layers.Dense(256, activation=\"relu\")(ip_red1_drop)\n",
    "ip_red2_drop = keras.layers.Dropout(0.2)(ip_red2)\n",
    "bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(128*2, return_sequences=True))\n",
    "a1 = bi_lstm(ip_red2_drop)\n",
    "\n",
    "# t_model1 = keras.models.Model(inputs=ip, outputs=a1)\n",
    "\n",
    "# attn_res = keras.layers.Attention()([a1, a1])\n",
    "\n",
    "# t_model2 = keras.models.Model(inputs=ip, outputs=attn_res)\n",
    "\n",
    "# res = tf.concat([a1, attn_res], axis = -1)\n",
    "a1 = keras.layers.Dropout(0.3)(a1)\n",
    "\n",
    "# x = keras.layers.Dense(512, activation=\"relu\")(res)\n",
    "# x=keras.layers.Dropout(0.2)(x)\n",
    "a2 = keras.layers.Dense(256, activation=\"relu\")(a1)\n",
    "a2=keras.layers.Dropout(0.1)(a2)\n",
    "a3 = keras.layers.Dense(128, activation=\"relu\")(a2)\n",
    "\n",
    "t_model1 = keras.models.Model(inputs=ip, outputs=a3)\n",
    "\n",
    "attn_res = keras.layers.Attention()([a3, a3])\n",
    "\n",
    "t_model2 = keras.models.Model(inputs=ip, outputs=attn_res)\n",
    "\n",
    "res = tf.concat([a3, attn_res], axis = -1)\n",
    "\n",
    "a4= keras.layers.Dense(64, activation=\"relu\")(res)\n",
    "op = keras.layers.Dense(len(vocab), activation=\"softmax\")(a4)\n",
    "\n",
    "model4=keras.models.Model(inputs=ip, outputs=op)\n",
    "model4.summary()\n",
    "model4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model4.fit(x_train,y_train, batch_size=64, epochs=100)\n",
    "# print(t_model1.summary(),end=\"\\n--------------\\n\")\n",
    "# print(t_model2.summary(),end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Attention visual</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "# model3.evaluate(x_test, y_test)\n",
    "sample = np.expand_dims(x_test[86], axis=0)\n",
    "\n",
    "# print(t_model1.predict(sample).shape)\n",
    "t1_preds = np.squeeze(t_model1.predict(sample))\n",
    "t1_preds.shape\n",
    "\n",
    "t2_preds = np.squeeze(t_model2.predict(sample))\n",
    "\n",
    "scores = np.zeros((75, 75))\n",
    "for i, frame_pred in enumerate(t1_preds):\n",
    "    attn_res = np.zeros((128))\n",
    "    for j, other_frame_pred in enumerate(t1_preds):\n",
    "        scores[i, j] = np.sum(frame_pred * other_frame_pred)\n",
    "    scores[i] = softmax(scores[i])\n",
    "    for j, other_frame_pred in enumerate(t1_preds):\n",
    "        attn_res += scores[i, j] * other_frame_pred\n",
    "#     print(t2_preds[i])\n",
    "#     print(t2_preds[i] - attn_res)\n",
    "#     break\n",
    "    \n",
    "    \n",
    "# print(t_model2.predict(sample).shape)\n",
    "print(scores)\n",
    "sns.heatmap(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>cer calc , cer without lm 0.122</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "def condense(preds):\n",
    "    global vocab\n",
    "    last_pred = -1\n",
    "    last_pred_freq = 0\n",
    "    fin_preds = []\n",
    "    for pred in preds:\n",
    "        if not pred == last_pred:\n",
    "            if not len(fin_preds) == 0:\n",
    "                fin_preds[len(fin_preds) - 1][1] = last_pred_freq + 1\n",
    "            last_pred_freq = 0\n",
    "            fin_preds.append([pred, 0])\n",
    "            last_pred = pred\n",
    "        else:\n",
    "            last_pred_freq += 1 \n",
    "    s=\"\"\n",
    "    for pred, freq in fin_preds:\n",
    "        s += \" \" + vocab[pred]\n",
    "    s = s.strip()    \n",
    "    return fin_preds, s\n",
    "\n",
    "ground_truth = []\n",
    "hypo = []\n",
    "fin_preds = []\n",
    "fin_labels = []\n",
    "\n",
    "all_preds = model4.predict(x_test)\n",
    "for sample_pred, label in zip(all_preds, y_test):\n",
    "#     sample = np.expand_dims(sample, axis=0)\n",
    "#     preds = np.squeeze(model3.predict(sample))\n",
    "    preds = np.argmax(sample_pred, axis=1)\n",
    "    labels = np.argmax(label, axis=1)\n",
    "    \n",
    "    fin_pred, s = condense(preds)\n",
    "    fin_preds.append(fin_pred)\n",
    "    hypo.append(\" \".join(list(s)))\n",
    "    \n",
    "    fin_label, s = condense(labels)\n",
    "    fin_labels.append(fin_label)\n",
    "    ground_truth.append(\" \".join(list(s)))\n",
    "    \n",
    "print(wer(ground_truth, hypo))\n",
    "#     print(preds,\"\\n\", condense(preds))\n",
    "#     print(labels,\"\\n\", condense(labels))\n",
    "    \n",
    "#     print(\"\\n----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [1,3,4, 5]:\n",
    "    fin_preds_x = fin_preds[x]\n",
    "    fin_labels_x = fin_labels[x]\n",
    "    for fin_pred in fin_preds_x:\n",
    "        print(vocab[fin_pred[0]], \", \", fin_pred[1], end=\"  \")\n",
    "    print()\n",
    "    for fin_pred in fin_labels_x:\n",
    "        print(vocab[fin_pred[0]], \", \", fin_pred[1], end=\"  \")\n",
    "    print(\"\\n\\n----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_preds[x, 27:36, vocab.index(\"three\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "static lambda with shallow fusion won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sent):\n",
    "    global vocab\n",
    "    res = np.zeros((1, 53))\n",
    "    for i, word in enumerate(sent.split()):\n",
    "        res[0, vocab.index(word)] = 1\n",
    "    return res\n",
    "\n",
    "def decode(pred):\n",
    "    pred = pred[0]\n",
    "    return vocab[np.argmax(pred)]\n",
    "\n",
    "def predict(model, lang_model, lmbda, x): #x.shape = (bs, 75, 2048)\n",
    "    m_preds = model.predict(x) #shape = (bs, 75, 53)\n",
    "    vid_preds = []\n",
    "    for i, vid_pred in enumerate(m_preds):\n",
    "        fin_preds = []   \n",
    "        mt_preds=[]\n",
    "        for frame_num, frame_pred in enumerate(vid_pred):\n",
    "            frame_pred_word = vocab[np.argmax(frame_pred)]            \n",
    "            if frame_num == 0:\n",
    "                fin_preds.append(frame_pred_word)                \n",
    "                mt_preds.append(frame_pred_word)\n",
    "                continue\n",
    "            if fin_preds[-1]==\"sil\" and len(fin_preds) > 1:\n",
    "                break\n",
    "            if frame_pred_word == \"sil\" and not len(fin_preds) == 1:\n",
    "                fin_preds.append(\"sil\")\n",
    "                mt_preds.append(\"sil\")\n",
    "                break\n",
    "            if not frame_pred_word == mt_preds[-1]:\n",
    "                mt_preds.append(frame_pred_word)            \n",
    "                lm_pred = lang_model.predict(encode(fin_preds[-1]))[0]                \n",
    "                assert lm_pred.shape == (53, )\n",
    "                sf_pred = np.log(frame_pred) + lmbda * np.log(lm_pred)\n",
    "                sf_pred_word = vocab[np.argmax(sf_pred)]\n",
    "                fin_preds.append(sf_pred_word)                \n",
    "        vid_preds.append(\" \".join(fin_preds).strip())\n",
    "    return vid_preds\n",
    "\n",
    "\n",
    "# test_fin_preds = predict(model4, lang_model, 1, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gt(labels): #(bs, 75, 53)\n",
    "    labels = np.argmax(labels, axis=2) #(bs, 75)\n",
    "    res = []\n",
    "    for vid_label in labels:\n",
    "        vid_res = [\"sil\"]\n",
    "        for word_index in vid_label:\n",
    "            word = vocab[word_index]\n",
    "            if not vid_res[-1] == word:\n",
    "                vid_res.append(word)\n",
    "        res.append(\" \".join(vid_res).strip())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x1ef25b629c8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_preds = model4.predict(x_train)\n",
    "lang_integ_x_m = np.zeros((x_train.shape[0]*7, len(vocab)))\n",
    "lang_integ_x_lang = np.zeros((x_train.shape[0]*7, len(vocab)))\n",
    "lang_integ_y = np.zeros((x_train.shape[0]*7, len(vocab)))\n",
    "\n",
    "for i, sample in enumerate(x_train):\n",
    "    for j, frame in sample:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fin_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "wer(test_sents, test_fin_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,5,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "for lmbda in np.arange(0,2,0.1):\n",
    "    print(lmbda,  wer(test_sents, predict(model4, lang_model, lmbda, x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15890751086281812"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(get_sent_from_preds(y_test), predict(model3, lang_model, 0, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: double_attention_250e_0.12_wer\\assets\n"
     ]
    }
   ],
   "source": [
    "model4.save(\"double_attention_250e_0.12_wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = keras.models.load_model(\"double_attention_250e_0.12_wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
